{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This document is meant to prepare the figures required for the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolatrix data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Initialize empty DataFrames to store the results\n",
    "df_max_prediction_1 = pd.DataFrame()\n",
    "df_max_prediction_0 = pd.DataFrame()\n",
    "\n",
    "# Your home directory path (change this to your actual path)\n",
    "home_dir = \"/projects/steiflab/archive/data/imaging/A138856A/NozzleImages\"\n",
    "\n",
    "# Traverse through each subdirectory in the home directory\n",
    "for root, dirs, files in os.walk(home_dir):\n",
    "    for file in files:\n",
    "        if file == \"LogFile.csv\":\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Read the logfile.csv file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Group by R and C, then find the row with the maximum Prediction for each group\n",
    "            df_max_pred = df.loc[df.groupby(['R', 'C'])['Prediction'].idxmax()]\n",
    "\n",
    "            # Filter for prediction = 1\n",
    "            df_pred_1 = df_max_pred[df_max_pred['Prediction'] == 1][['R', 'C', 'Prediction']]\n",
    "            df_max_prediction_1 = pd.concat([df_max_prediction_1, df_pred_1], ignore_index=True)\n",
    "            \n",
    "            # Filter for prediction = 0\n",
    "            df_pred_0 = df_max_pred[df_max_pred['Prediction'] == 0][['R', 'C', 'Prediction']]\n",
    "            df_max_prediction_0 = pd.concat([df_max_prediction_0, df_pred_0], ignore_index=True)\n",
    "\n",
    "\n",
    "# Rename columns in df_max_prediction_1 and df_max_prediction_0 to 'row', 'col', and 'prediction'\n",
    "df_max_prediction_1.columns = ['row', 'col', 'prediction']\n",
    "df_max_prediction_0.columns = ['row', 'col', 'prediction']\n",
    "\n",
    "# Concatenate the two dataframes together\n",
    "df_combined_predictions = pd.concat([df_max_prediction_1, df_max_prediction_0], ignore_index=True)\n",
    "\n",
    "# Detect and resolve duplicates in df_combined_predictions by choosing the maximum prediction\n",
    "df_combined_predictions = df_combined_predictions.groupby(['row', 'col'], as_index=False).agg({'prediction': 'max'})\n",
    "\n",
    "# Check for duplicates based on the combination of 'row' and 'col'\n",
    "duplicates = df_combined_predictions[df_combined_predictions.duplicated(subset=['row', 'col'], keep=False)]\n",
    "\n",
    "# Print the duplicated rows\n",
    "if not duplicates.empty:\n",
    "    print(\"Found duplicates in the df_combined_predictions df  of 'row' and 'col':\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicates found in the df_combined_predictions df  of 'row' and 'col'.\")\n",
    "\n",
    "# Step 3: Read the new file (A138856.tsv) and prepare the new_df\n",
    "new_df = pd.read_csv('/projects/steiflab/archive/data/wgs/single_cell/internal/A138856/merge/metadata.tsv', delimiter = '\\t')\n",
    "print(f\"The metadata has shape {new_df.shape}\")\n",
    "\n",
    "# Check for duplicates based on the combination of 'row' and 'col'\n",
    "duplicates = new_df[new_df.duplicated(subset=['row', 'col'], keep=False)]\n",
    "\n",
    "# Print the duplicated rows\n",
    "if not duplicates.empty:\n",
    "    print(\"Found duplicates in the new df  of 'row' and 'col':\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicates found in the new df  of 'row' and 'col'.\")\n",
    "\n",
    "# Merge the combined dataframe with new_df, ensuring that all rows in new_df are retained\n",
    "merged_df = pd.merge(new_df, df_combined_predictions, how='left', on=['row', 'col'])\n",
    "\n",
    "combination_counts = new_df.groupby(['experimental_condition', 'cell_condition']).size().reset_index(name='counts')\n",
    "combination_counts.sort_values(by='counts', ascending=False, inplace=True)\n",
    "\n",
    "print(combination_counts)\n",
    "\n",
    "combination_counts_2 = merged_df[~merged_df['prediction'].isna()].groupby(['experimental_condition', 'cell_condition']).size().reset_index(name='counts')\n",
    "combination_counts_2.sort_values(by='counts', ascending=False, inplace=True)\n",
    "\n",
    "print(\"got rif of the prediction is NA\")\n",
    "print(combination_counts_2)\n",
    "\n",
    "isolatrix_df = merged_df[~merged_df['prediction'].isna()]\n",
    "\n",
    "isolatrix_df = isolatrix_df[['row', 'col', 'experimental_condition','cell_condition', 'prediction']]\n",
    "isolatrix_df.columns\n",
    "\n",
    "# Check for duplicates based on the combination of 'row' and 'col'\n",
    "duplicates = isolatrix_df[isolatrix_df.duplicated(subset=['row', 'col'], keep=False)]\n",
    "\n",
    "# Print the duplicated rows\n",
    "if not duplicates.empty:\n",
    "    print(\"Found duplicates in the isolatrix_df df  of 'row' and 'col':\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicates found in the isolatrix_df df  of 'row' and 'col'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cellenONE data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_path = \"/projects/steiflab/archive/data/wgs/single_cell/internal/A138856/merge/metadata.tsv\"\n",
    "new_df = pd.read_csv('/projects/steiflab/archive/data/wgs/single_cell/internal/A138856/merge/metadata.tsv', delimiter = '\\t')\n",
    "print(f\"The metadata has shape {new_df.shape}\")\n",
    "\n",
    "# Filter new_df to keep only rows where 'experimental_condition' is 'CellenONE'\n",
    "cellenone_df = new_df[new_df['experimental_condition'] == 'CellenONE'].copy()\n",
    "\n",
    "combination_counts_2 = cellenone_df.groupby(['experimental_condition', 'cell_condition']).size().reset_index(name='counts')\n",
    "combination_counts_2.sort_values(by='counts', ascending=False, inplace=True)\n",
    "\n",
    "print(combination_counts_2)\n",
    "\n",
    "# Add a 'prediction' column with value 1 (as all predictions for CellenONE are considered as 1)\n",
    "cellenone_df['prediction'] = 1\n",
    "\n",
    "# Keep only the 'row', 'col', and 'prediction' columns\n",
    "cellenone_df = cellenone_df[['row', 'col',  'experimental_condition','cell_condition','prediction']]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(f\"The cellenone_df has shape {cellenone_df.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([isolatrix_df, cellenone_df], ignore_index=True)\n",
    "# Check for duplicates based on the combination of 'row' and 'col'\n",
    "duplicates = combined_df[combined_df.duplicated(subset=['row', 'col'], keep=False)]\n",
    "\n",
    "# Print the duplicated rows\n",
    "if not duplicates.empty:\n",
    "    print(\"Found duplicates in the combination of 'row' and 'col':\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicates found in the combination of 'row' and 'col'.\")\n",
    "\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the conditions for Isolatrix and CellenONE\n",
    "isolatrix_conditions = ['Iso', 'Iso10']\n",
    "cellenone_condition = 'CellenONE'\n",
    "\n",
    "# Create a new column 'technology' to categorize rows as Isolatrix or CellenONE\n",
    "combined_df['technology'] = combined_df['experimental_condition'].apply(\n",
    "    lambda x: 'Isolatrix' if x in isolatrix_conditions else 'CellenONE' if x == cellenone_condition else None\n",
    ")\n",
    "\n",
    "# Drop rows where 'technology' is None (i.e., those that are neither Isolatrix nor CellenONE)\n",
    "combined_df = combined_df.dropna(subset=['technology'])\n",
    "\n",
    "# Define the ground truth: 1 for LiveCell, 0 for all other conditions\n",
    "combined_df['ground_truth'] = combined_df['cell_condition'].apply(lambda x: 1 if x == 'LiveCell' else 0)\n",
    "\n",
    "# Verify total counts for Isolatrix and CellenONE\n",
    "isolatrix_count = combined_df[combined_df['technology'] == 'Isolatrix'].shape[0]\n",
    "cellenone_count = combined_df[combined_df['technology'] == 'CellenONE'].shape[0]\n",
    "\n",
    "print(f\"Total number of samples for Isolatrix: {isolatrix_count}\")\n",
    "print(f\"Total number of samples for CellenONE: {cellenone_count}\")\n",
    "\n",
    "# Create the confusion matrix for Isolatrix\n",
    "iso_df = combined_df[combined_df['technology'] == 'Isolatrix']\n",
    "iso_cm = confusion_matrix(iso_df['ground_truth'], iso_df['prediction'])\n",
    "\n",
    "# Create the confusion matrix for CellenONE\n",
    "cellenone_df = combined_df[combined_df['technology'] == 'CellenONE']\n",
    "cellenone_cm = confusion_matrix(cellenone_df['ground_truth'], cellenone_df['prediction'])\n",
    "\n",
    "# Function to calculate percentages relative to total sample size\n",
    "# Function to calculate percentages relative to total sample size and format as integers\n",
    "def add_percentage_labels_total(cm, total_count):\n",
    "    cm_perc = cm / total_count * 100\n",
    "    labels = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            count = cm[i, j]\n",
    "            perc = int(cm_perc[i, j])  # Convert percentage to an integer\n",
    "            labels.append(f\"{count}\\n({perc}%)\")\n",
    "    return np.array(labels).reshape(cm.shape)\n",
    "\n",
    "\n",
    "# Generate labels with percentages for Isolatrix\n",
    "iso_labels = add_percentage_labels_total(iso_cm, isolatrix_count)\n",
    "\n",
    "# Generate labels with percentages for CellenONE\n",
    "cellenone_labels = add_percentage_labels_total(cellenone_cm, cellenone_count)\n",
    "\n",
    "# Plot the confusion matrix for Isolatrix\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(iso_cm, annot=iso_labels, fmt='', cmap='Blues', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 16},  # Adjust the font size here\n",
    "            xticklabels=['Predicted Negative (class 0)', 'Predicted Positive (class 1)'],\n",
    "            yticklabels=['True Negative', 'True Positive'])\n",
    "plt.title('Isolatrix Confusion Matrix', fontsize=14)\n",
    "\n",
    "# Plot the confusion matrix for CellenONE\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cellenone_cm, annot=cellenone_labels, fmt='', cmap='YlOrBr', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 16},  # Adjust the font size here\n",
    "            xticklabels=['Predicted Negative (discarded)', 'Predicted Positive (isolated)'],\n",
    "            yticklabels=['True Negative', 'True Positive'])\n",
    "plt.title('CellenONE Confusion Matrix', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/projects/steiflab/scratch/leli/A138856A/plots/confusion_matrix.svg\", format=\"svg\")\n",
    "plt.savefig(\"/projects/steiflab/scratch/leli/A138856A/plots/confusion_matrix.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the conditions for Isolatrix and CellenONE\n",
    "isolatrix_conditions = ['Iso', 'Iso10']\n",
    "cellenone_condition = 'CellenONE'\n",
    "\n",
    "# Create a new column 'technology' to categorize rows as Isolatrix or CellenONE\n",
    "combined_df['technology'] = combined_df['experimental_condition'].apply(\n",
    "    lambda x: 'Isolatrix' if x in isolatrix_conditions else 'CellenONE' if x == cellenone_condition else None\n",
    ")\n",
    "\n",
    "# Drop rows where 'technology' is None (i.e., those that are neither Isolatrix nor CellenONE)\n",
    "# combined_df = combined_df.dropna(subset=['technology'])\n",
    "\n",
    "# Define the ground truth: 1 for LiveCell, 0 for all other conditions\n",
    "combined_df['ground_truth'] = combined_df['cell_condition'].apply(lambda x: 1 if x == 'LiveCell' else 0)\n",
    "\n",
    "# Verify total counts for Isolatrix and CellenONE\n",
    "isolatrix_count = combined_df[combined_df['technology'] == 'Isolatrix'].shape[0]\n",
    "cellenone_count = combined_df[combined_df['technology'] == 'CellenONE'].shape[0]\n",
    "\n",
    "print(f\"Total number of samples for Isolatrix: {isolatrix_count}\")\n",
    "print(f\"Total number of samples for CellenONE: {cellenone_count}\")\n",
    "\n",
    "# Create the confusion matrix for Isolatrix\n",
    "iso_df = combined_df[combined_df['technology'] == 'Isolatrix']\n",
    "iso_cm = confusion_matrix(iso_df['ground_truth'], iso_df['prediction'])\n",
    "\n",
    "# Create the confusion matrix for CellenONE\n",
    "cellenone_df = combined_df[combined_df['technology'] == 'CellenONE']\n",
    "cellenone_cm = confusion_matrix(cellenone_df['ground_truth'], cellenone_df['prediction'])\n",
    "\n",
    "# Function to calculate percentages relative to total sample size\n",
    "# Function to calculate percentages relative to total sample size and format as integers\n",
    "def add_percentage_labels_total(cm, total_count):\n",
    "    cm_perc = cm / total_count * 100\n",
    "    labels = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            count = cm[i, j]\n",
    "            perc = int(cm_perc[i, j])  # Convert percentage to an integer\n",
    "            labels.append(f\"{count}\\n({perc}%)\")\n",
    "    return np.array(labels).reshape(cm.shape)\n",
    "\n",
    "\n",
    "# Generate labels with percentages for Isolatrix\n",
    "iso_labels = add_percentage_labels_total(iso_cm, isolatrix_count)\n",
    "\n",
    "# Generate labels with percentages for CellenONE\n",
    "cellenone_labels = add_percentage_labels_total(cellenone_cm, cellenone_count)\n",
    "\n",
    "# Plot the confusion matrix for Isolatrix\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(iso_cm, annot=iso_labels, fmt='', cmap='Blues', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 24},  # Adjust the font size here\n",
    "            xticklabels=['Predicted Negative (class 0)', 'Predicted Positive (class 1)'],\n",
    "            yticklabels=['True Negative', 'True Positive'])\n",
    "plt.title('Isolatrix Confusion Matrix', fontsize=14)\n",
    "\n",
    "# Plot the confusion matrix for CellenONE\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Mask for the \"Predicted Negative\" column in CellenONE matrix\n",
    "mask = np.zeros_like(cellenone_cm, dtype=bool)\n",
    "mask[:, 0] = True  # Mask the \"Predicted Negative\" column\n",
    "\n",
    "# Plot the CellenONE confusion matrix without the \"Predicted Negative\" values\n",
    "sns.heatmap(cellenone_cm, annot=cellenone_labels, fmt='', cmap='YlOrBr', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 24},  # Adjust the font size here\n",
    "            xticklabels=['Predicted Negative (discarded)', 'Predicted Positive (isolated)'],\n",
    "            yticklabels=['True Negative', 'True Positive'], mask=mask)\n",
    "\n",
    "# Overlay the dark gray boxes for the Predicted Negative column\n",
    "sns.heatmap(np.zeros_like(cellenone_cm), mask=~mask, cbar=False, square=True, cmap='Greys',\n",
    "            xticklabels=['Predicted Negative (discarded)', 'Predicted Positive (isolated)'],\n",
    "            yticklabels=['True Negative', 'True Positive'],)\n",
    "\n",
    "plt.title('CellenONE Confusion Matrix', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/projects/steiflab/scratch/leli/A138856A/plots/confusion_matrix_wo_discarded.svg\", format=\"svg\")\n",
    "plt.savefig(\"/projects/steiflab/scratch/leli/A138856A/plots/confusion_matrix_wo_discarded.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is for the isolaltrix where we try to correct it to make the weighted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the conditions for Isolatrix\n",
    "isolatrix_conditions = ['Iso', 'Iso10']\n",
    "\n",
    "# Create a new column 'technology' to categorize rows as Isolatrix\n",
    "combined_df['technology'] = combined_df['experimental_condition'].apply(\n",
    "    lambda x: 'Isolatrix' if x in isolatrix_conditions else None\n",
    ")\n",
    "\n",
    "# Drop rows where 'technology' is None (i.e., those that are not Isolatrix)\n",
    "combined_df = combined_df.dropna(subset=['technology'])\n",
    "\n",
    "# Define the ground truth: 1 for LiveCell, 0 for all other conditions\n",
    "combined_df['ground_truth'] = combined_df['cell_condition'].apply(lambda x: 1 if x == 'LiveCell' else 0)\n",
    "\n",
    "# Verify total count for Isolatrix\n",
    "isolatrix_count = combined_df[combined_df['technology'] == 'Isolatrix'].shape[0]\n",
    "class_0_sample_count = combined_df[(combined_df['technology'] == 'Isolatrix') & (combined_df['prediction'] == 0)].shape[0]\n",
    "class_1_sample_count = combined_df[(combined_df['technology'] == 'Isolatrix') & (combined_df['prediction'] == 1)].shape[0]\n",
    "\n",
    "print(f\"Total number of samples for Isolatrix: {isolatrix_count}\")\n",
    "print(f\"Class 0 sample count: {class_0_sample_count}\")\n",
    "print(f\"Class 1 sample count: {class_1_sample_count}\")\n",
    "\n",
    "# Create the confusion matrix for Isolatrix\n",
    "iso_df = combined_df[combined_df['technology'] == 'Isolatrix']\n",
    "iso_cm = confusion_matrix(iso_df['ground_truth'], iso_df['prediction'])\n",
    "\n",
    "# Define population proportions\n",
    "total_population = 3311\n",
    "class_0_population = 1824\n",
    "class_1_population = 1189\n",
    "\n",
    "# Calculate sample proportions\n",
    "class_0_sample_proportion = class_0_sample_count / isolatrix_count\n",
    "class_1_sample_proportion = class_1_sample_count / isolatrix_count\n",
    "\n",
    "# Calculate class weights based on population distribution\n",
    "class_weight_0 = (class_0_population / total_population) / class_0_sample_proportion\n",
    "class_weight_1 = (class_1_population / total_population) / class_1_sample_proportion\n",
    "\n",
    "# Create a weight matrix\n",
    "weight_matrix = np.array([[class_weight_0, class_weight_1], [class_weight_0, class_weight_1]])\n",
    "\n",
    "# Apply the weights to the confusion matrix\n",
    "# Add a constant of 1 to ensure all values in the confusion matrix are nonzero for the weighted matrix\n",
    "weighted_iso_cm = (iso_cm + 1) * weight_matrix\n",
    "\n",
    "# Calculate metrics for the original confusion matrix\n",
    "original_accuracy = accuracy_score(iso_df['ground_truth'], iso_df['prediction'])\n",
    "original_precision = precision_score(iso_df['ground_truth'], iso_df['prediction'])\n",
    "original_recall = recall_score(iso_df['ground_truth'], iso_df['prediction'])\n",
    "original_f1 = f1_score(iso_df['ground_truth'], iso_df['prediction'])\n",
    "\n",
    "# Calculate metrics for the weighted confusion matrix\n",
    "# Note: For weighted metrics, we need to manually calculate based on the weighted confusion matrix\n",
    "weighted_tp = weighted_iso_cm[1, 1]\n",
    "weighted_fp = weighted_iso_cm[0, 1]\n",
    "weighted_fn = weighted_iso_cm[1, 0]\n",
    "weighted_tn = weighted_iso_cm[0, 0]\n",
    "\n",
    "weighted_accuracy = (weighted_tp + weighted_tn) / weighted_iso_cm.sum()\n",
    "weighted_precision = weighted_tp / (weighted_tp + weighted_fp)\n",
    "weighted_recall = weighted_tp / (weighted_tp + weighted_fn)\n",
    "weighted_f1 = 2 * (weighted_precision * weighted_recall) / (weighted_precision + weighted_recall)\n",
    "\n",
    "# Print the metrics for comparison\n",
    "print(\"Original Confusion Matrix Metrics:\")\n",
    "print(f\"Accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Precision: {original_precision:.4f}\")\n",
    "print(f\"Recall: {original_recall:.4f}\")\n",
    "print(f\"F1 Score: {original_f1:.4f}\\n\")\n",
    "\n",
    "print(\"Weighted Confusion Matrix Metrics:\")\n",
    "print(f\"Accuracy: {weighted_accuracy:.4f}\")\n",
    "print(f\"Precision: {weighted_precision:.4f}\")\n",
    "print(f\"Recall: {weighted_recall:.4f}\")\n",
    "print(f\"F1 Score: {weighted_f1:.4f}\")\n",
    "\n",
    "# Function to calculate percentages relative to total sample size and format as integers\n",
    "def add_percentage_labels_total(cm, total_count):\n",
    "    cm_perc = cm / total_count * 100\n",
    "    labels = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            count = cm[i, j]\n",
    "            if count > 1: \n",
    "                count = int(count)\n",
    "            else: \n",
    "                count = round(count, 2)\n",
    "            perc = int(cm_perc[i, j])  # Convert percentage to an integer\n",
    "            labels.append(f\"{count}\\n({perc}%)\")\n",
    "    return np.array(labels).reshape(cm.shape)\n",
    "\n",
    "# Generate labels with percentages for the original confusion matrix\n",
    "original_iso_labels = add_percentage_labels_total(iso_cm, isolatrix_count)\n",
    "\n",
    "# Generate labels with percentages for the weighted confusion matrix\n",
    "weighted_iso_labels = add_percentage_labels_total(weighted_iso_cm, isolatrix_count)\n",
    "\n",
    "# Plot the original and weighted confusion matrices side by side\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Original Confusion Matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(iso_cm, annot=original_iso_labels, fmt='', cmap='Blues', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 16},  # Adjust the font size here\n",
    "            xticklabels=['Predicted Negative (class 0)', 'Predicted Positive (class 1)'],\n",
    "            yticklabels=['True Negative', 'True Positive'])\n",
    "plt.title('Isolatrix Confusion Matrix', fontsize=14)\n",
    "\n",
    "# Weighted Confusion Matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(weighted_iso_cm, annot=weighted_iso_labels, fmt='', cmap='Blues', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 16},  # Adjust the font size here\n",
    "            xticklabels=['Predicted Negative (class 0)', 'Predicted Positive (class 1)'],\n",
    "            yticklabels=['True Negative', 'True Positive'])\n",
    "plt.title('Weighted Isolatrix Confusion Matrix', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/projects/steiflab/scratch/leli/A138856A/plots/compare_confusion_matrices.svg\", format=\"svg\")\n",
    "plt.savefig(\"/projects/steiflab/scratch/leli/A138856A/plots/compare_confusion_matrices.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "combined_df = pd.read_csv(\"/projects/steiflab/scratch/leli/A138856A/final_combined_df.csv\")\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of iterations and the number of wells to sample\n",
    "iterations = 1000\n",
    "num_wells = 384\n",
    "\n",
    "# Initialize lists to store the results\n",
    "results_isolatrix = {'True Positives': [], 'False Positives': [], 'True Negatives': [], 'False Negatives': []}\n",
    "results_cellenone = {'True Positives': [], 'False Positives': [], 'True Negatives': [], 'False Negatives': []}\n",
    "\n",
    "# Iterate 1000 times to perform random sampling and calculate metrics\n",
    "for _ in range(iterations):\n",
    "    # Sample 384 wells for Isolatrix\n",
    "    sampled_isolatrix = combined_df[combined_df['technology'] == 'Isolatrix'].sample(n=num_wells, replace=True)\n",
    "    tp_isolatrix = len(sampled_isolatrix[(sampled_isolatrix['ground_truth'] == 1) & (sampled_isolatrix['prediction'] == 1)])\n",
    "    fp_isolatrix = len(sampled_isolatrix[(sampled_isolatrix['ground_truth'] == 0) & (sampled_isolatrix['prediction'] == 1)])\n",
    "    tn_isolatrix = len(sampled_isolatrix[(sampled_isolatrix['ground_truth'] == 0) & (sampled_isolatrix['prediction'] == 0)])\n",
    "    fn_isolatrix = len(sampled_isolatrix[(sampled_isolatrix['ground_truth'] == 1) & (sampled_isolatrix['prediction'] == 0)])\n",
    "    \n",
    "    results_isolatrix['True Positives'].append(tp_isolatrix)\n",
    "    results_isolatrix['False Positives'].append(fp_isolatrix)\n",
    "    results_isolatrix['True Negatives'].append(tn_isolatrix)\n",
    "    results_isolatrix['False Negatives'].append(fn_isolatrix)\n",
    "\n",
    "    # Sample 384 wells for CellenONE\n",
    "    sampled_cellenone = combined_df[combined_df['technology'] == 'CellenONE'].sample(n=num_wells, replace=True)\n",
    "    tp_cellenone = len(sampled_cellenone[(sampled_cellenone['ground_truth'] == 1) & (sampled_cellenone['prediction'] == 1)])\n",
    "    fp_cellenone = len(sampled_cellenone[(sampled_cellenone['ground_truth'] == 0) & (sampled_cellenone['prediction'] == 1)])\n",
    "    tn_cellenone = len(sampled_cellenone[(sampled_cellenone['ground_truth'] == 0) & (sampled_cellenone['prediction'] == 0)])\n",
    "    fn_cellenone = len(sampled_cellenone[(sampled_cellenone['ground_truth'] == 1) & (sampled_cellenone['prediction'] == 0)])\n",
    "    \n",
    "    results_cellenone['True Positives'].append(tp_cellenone)\n",
    "    results_cellenone['False Positives'].append(fp_cellenone)\n",
    "    results_cellenone['True Negatives'].append(tn_cellenone)\n",
    "    results_cellenone['False Negatives'].append(fn_cellenone)\n",
    "\n",
    "# Calculate the mean values across iterations\n",
    "mean_results_isolatrix = {key: np.mean(value) for key, value in results_isolatrix.items()}\n",
    "mean_results_cellenone = {key: np.mean(value) for key, value in results_cellenone.items()}\n",
    "\n",
    "# Plotting\n",
    "metrics = ['True Positives', 'False Positives', 'True Negatives', 'False Negatives']\n",
    "isolatrix_counts = [mean_results_isolatrix[metric] for metric in metrics]\n",
    "cellenone_counts = [mean_results_cellenone[metric] for metric in metrics]\n",
    "\n",
    "# Adjust zero bars to appear slightly higher (e.g., at 5) but display as 0\n",
    "adjusted_isolatrix_counts = [2 if count == 0 else count for count in isolatrix_counts]\n",
    "adjusted_cellenone_counts = [2 if count == 0 else count for count in cellenone_counts]\n",
    "\n",
    "x = np.arange(len(metrics))  # the label locations\n",
    "bar_width = 0.35  # the width of the bars\n",
    "offset = 0.02  # This adds a small gap between the bars\n",
    "\n",
    "# Create the bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the bars for Isolatrix\n",
    "bars1 = ax.bar(x - bar_width/2 - offset, adjusted_isolatrix_counts, bar_width, \n",
    "               label='Isolatrix', color='#5A79A5', edgecolor='#334F73', linewidth=2)\n",
    "\n",
    "# Plot the bars for CellenONE\n",
    "bars2 = ax.bar(x + bar_width/2 + offset, adjusted_cellenone_counts, bar_width, \n",
    "               label='CellenONE', color='#F3E197', edgecolor='#BBA653', linewidth=2)\n",
    "\n",
    "# Add the count values and percentages on top of the bars for Isolatrix\n",
    "for bar, original_value in zip(bars1, isolatrix_counts):\n",
    "    yval = bar.get_height()\n",
    "    percentage = (original_value / num_wells) * 100\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, max(yval + 1, 1), f'{int(original_value):,}\\n({int(percentage):}%)', \n",
    "            ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Add the count values and percentages on top of the bars for CellenONE\n",
    "for bar, original_value in zip(bars2, cellenone_counts):\n",
    "    yval = bar.get_height()\n",
    "    percentage = (original_value / num_wells) * 100\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, max(yval + 1, 1), f'{int(original_value):,}\\n({int(percentage):}%)', \n",
    "            ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Expected Performance (n = 384)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# Adjust y-lim to ensure bars and text are not cut off\n",
    "ax.set_ylim(0, max(max(adjusted_isolatrix_counts), max(adjusted_cellenone_counts)) + 30)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"/projects/steiflab/scratch/leli/A138856A/plots/exp_performance.svg\", format=\"svg\")\n",
    "plt.savefig(\"/projects/steiflab/scratch/leli/A138856A/plots/exp_performance.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get value ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the previous dataframes have been created:\n",
    "# merged_df_pred_1 (Isolatrix with Prediction = 1 and total_reads > 113014 + 72160)\n",
    "# merged_df_pred_0 (Isolatrix with Prediction = 0 and total_reads <= 113014 + 72160)\n",
    "# merged_isolated_metadata_df (CellenONE without Prediction column)\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "#np.random.seed(364599)\n",
    "sampling_frac = 0.2\n",
    "\n",
    "# Step 1: Add a 'Prediction' column with value 1 for merged_isolated_metadata_df\n",
    "merged_isolated_metadata_df['Prediction'] = 1\n",
    "\n",
    "# Step 2: Combine the Isolatrix dataframes into one\n",
    "isolatrix_combined_df = pd.concat([merged_df_pred_1, merged_df_pred_0], ignore_index=True)\n",
    "\n",
    "isolatrix_combined_df = isolatrix_combined_df.sample(n=400)\n",
    "merged_isolated_metadata_df = merged_isolated_metadata_df.sample(n=400)\n",
    "print(f\"the original dataframe is of size isolatrix {isolatrix_combined_df.shape} and the cellenone is {merged_isolated_metadata_df.shape}\")\n",
    "\n",
    "# Step 3: Determine the minimum number of positives (actual single cells) from each technology\n",
    "isolatrix_positives = isolatrix_combined_df[isolatrix_combined_df['total_reads'] > 113014 + 72160]\n",
    "isolatrix_negatives = isolatrix_combined_df[isolatrix_combined_df['total_reads'] <= 113014 + 72160]\n",
    "\n",
    "cellenone_positives = merged_isolated_metadata_df[merged_isolated_metadata_df['total_reads'] > 113014 + 72160]\n",
    "cellenone_negatives = merged_isolated_metadata_df[merged_isolated_metadata_df['total_reads'] <= 113014 + 72160]\n",
    "\n",
    "sample_size = min(len(isolatrix_positives), len(isolatrix_negatives), len(cellenone_positives), len(cellenone_negatives))\n",
    "print(f\"The sample size is {sample_size}, {[len(isolatrix_positives), len(isolatrix_negatives), len(cellenone_positives), len(cellenone_negatives)]}\")\n",
    "\n",
    "# Step 4: Randomly pick out a sample of positives (actual single cells) for each technology\n",
    "'''isolatrix_positives = isolatrix_positives.sample(frac = sampling_frac) #n = 132)#\n",
    "isolatrix_negatives = isolatrix_negatives.sample(frac = sampling_frac) #n = 25)\n",
    "cellenone_positives = cellenone_positives.sample(frac = sampling_frac) #n = 132)\n",
    "cellenone_negatives = cellenone_negatives.sample(frac = sampling_frac) #n = 25)#'''\n",
    "\n",
    "# Step 5: For each technology, count how many of the positives have a Prediction of 1 and 0\n",
    "isolatrix_tp_count = len(isolatrix_positives[isolatrix_positives['Prediction'] == 1])\n",
    "isolatrix_fn_count = len(isolatrix_positives[isolatrix_positives['Prediction'] == 0])\n",
    "isolatrix_tn_count = len(isolatrix_negatives[isolatrix_negatives['Prediction'] == 0])\n",
    "isolatrix_fp_count = len(isolatrix_negatives[isolatrix_negatives['Prediction'] == 1])\n",
    "\n",
    "cellenone_tp_count = len(cellenone_positives[cellenone_positives['Prediction'] == 1])\n",
    "cellenone_fn_count = len(cellenone_positives[cellenone_positives['Prediction'] == 0])\n",
    "cellenone_tn_count = len(cellenone_negatives[cellenone_negatives['Prediction'] == 0])\n",
    "cellenone_fp_count = len(cellenone_negatives[cellenone_negatives['Prediction'] == 1])\n",
    "\n",
    "# Print out the results\n",
    "print(\"\\nIsolatrix Results:\")\n",
    "print(f\"True Positives (TP): {isolatrix_tp_count}\")\n",
    "print(f\"False Negatives (FN): {isolatrix_fn_count}\")\n",
    "print(f\"True Negatives (TN): {isolatrix_tn_count}\")\n",
    "print(f\"False Positives (FP): {isolatrix_fp_count}\")\n",
    "\n",
    "print(\"\\nCellenONE Results:\")\n",
    "print(f\"True Positives (TP): {cellenone_tp_count}\")\n",
    "print(f\"False Negatives (FN): {cellenone_fn_count}\")\n",
    "print(f\"True Negatives (TN): {cellenone_tn_count}\")\n",
    "print(f\"False Positives (FP): {cellenone_fp_count}\")\n",
    "\n",
    "print(f\"True Positives (TP): {cellenone_positives[cellenone_positives['Prediction'] == 1]}\")\n",
    "print(f\"False Positives (FP): {cellenone_negatives[cellenone_negatives['Prediction'] == 1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT NOT SELECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cellenone_fn_count = None  # FN for CellenONE will be replaced with text\n",
    "\n",
    "# Data for the bar chart\n",
    "metrics = ['True Positives (TP)', 'False Positives (FP)', 'False Negatives (FN)']\n",
    "isolatrix_counts = [isolatrix_tp_count, isolatrix_fp_count, isolatrix_fn_count]\n",
    "cellenone_counts = [cellenone_tp_count, cellenone_fp_count, 0]  # FN is set to 0, and we'll replace it with text\n",
    "\n",
    "x = np.arange(len(metrics))  # the label locations\n",
    "bar_width = 0.35  # the width of the bars\n",
    "\n",
    "# Create the bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Adjust the offset to create space between the bars\n",
    "offset = 0.02  # This adds a small gap between the bars\n",
    "\n",
    "# Plot the bars for Isolatrix with a blue color and a darker blue border\n",
    "bars1 = ax.bar(x - bar_width/2 - offset, isolatrix_counts, bar_width, \n",
    "               label='Isolatrix', color='#5A79A5', edgecolor='#334F73', linewidth=2)\n",
    "\n",
    "# Plot the bars for CellenONE with a yellow color and a darker yellow border\n",
    "bars2 = ax.bar(x + bar_width/2 + offset, cellenone_counts, bar_width, \n",
    "               label='CellenONE', color='#F3E197', edgecolor='#BBA653', linewidth=2)\n",
    "\n",
    "# Replace the CellenONE FN bar with text\n",
    "for i in range(len(cellenone_counts)):\n",
    "    if cellenone_counts[i] == 0 and metrics[i] == 'False Negatives (FN)':\n",
    "        ax.text(x[i] + bar_width/2 + offset, 0, '  * CellenONE: No ground truth for validation', \n",
    "                ha='center', va='bottom', fontsize=12, color='black', rotation=90, style='italic')\n",
    "\n",
    "# Add the count values on top of the bars for Isolatrix\n",
    "for bar in bars1:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Add the count values on top of the bars for CellenONE\n",
    "i = 0\n",
    "for bar in bars2:\n",
    "    if i == 2: continue\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=12)\n",
    "    i += 1\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Evaluation of Isolatrix and CellenONE Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv(\"/projects/steiflab/scratch/leli/A138856A/feature_metadata.csv\")\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize new columns in combined_df\n",
    "combined_df['track_ID'] = pd.NA\n",
    "combined_df['subdirectory_name'] = pd.NA\n",
    "combined_df['area'] = pd.NA\n",
    "combined_df['circularity'] = pd.NA\n",
    "combined_df['intensity'] = pd.NA\n",
    "combined_df['energy'] = pd.NA\n",
    "combined_df['intensity_not_norm'] = pd.NA\n",
    "\n",
    "# Your home directory path (change this to your actual directory)\n",
    "home_dir = \"/projects/steiflab/scratch/leli/A138856A\"  # Replace with your actual directory\n",
    "\n",
    "# Read the features_metadata.csv file\n",
    "features_metadata_df = pd.read_csv(\"/projects/steiflab/scratch/leli/A138856A/feature_metadata.csv\")\n",
    "\n",
    "# Iterate through each subfolder in the home directory\n",
    "for subfolder in os.listdir(home_dir):\n",
    "    subfolder_path = os.path.join(home_dir, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Process subfolders containing \"dropRun\"\n",
    "        track_to_well_path = os.path.join(subfolder_path, \"track_to_well\", \"track_to_well_pp.csv\")\n",
    "        \n",
    "        if os.path.exists(track_to_well_path):\n",
    "            # Read the track_to_well_pp.csv file\n",
    "            track_to_well_df = pd.read_csv(track_to_well_path)\n",
    "            \n",
    "            # Filter to keep only positive, nonzero track IDs\n",
    "            track_to_well_df = track_to_well_df[track_to_well_df['track_ID'] > 0]\n",
    "            \n",
    "            # Merge track_to_well_df with features_metadata_df on 'track_ID' (track_to_well) and 'track_id' (features_metadata)\n",
    "            merged_df = pd.merge(track_to_well_df, features_metadata_df, left_on='track_ID', right_on='track_id', how='left')\n",
    "            \n",
    "            # Check for rows in track_to_well_df that do not have associated entries in features_metadata_df\n",
    "            missing_metadata = merged_df[merged_df['track_id'].isna()]\n",
    "            if not missing_metadata.empty:\n",
    "                print(f\"Warning: {len(missing_metadata)} rows in 'track_to_well_pp.csv' with positive track IDs do not have associated entries in 'features_metadata.csv' for subdirectory: {subfolder}\")\n",
    "                print(\"Missing Track IDs:\", missing_metadata['track_ID'].tolist())\n",
    "            \n",
    "            # Now link the merged_df with combined_df based on 'row' and 'col'\n",
    "            for _, row in merged_df.iterrows():\n",
    "                combined_index = combined_df[(combined_df['row'] == row['row']) & (combined_df['col'] == row['col'])].index\n",
    "                if not combined_index.empty:\n",
    "                    combined_df.loc[combined_index, 'track_ID'] = row['track_ID']\n",
    "                    combined_df.loc[combined_index, 'subdirectory_name'] = subfolder\n",
    "                    combined_df.loc[combined_index, 'area'] = row['area']\n",
    "                    combined_df.loc[combined_index, 'circularity'] = row['circularity']\n",
    "                    combined_df.loc[combined_index, 'intensity'] = row['intensity']\n",
    "                    combined_df.loc[combined_index, 'energy'] = row['energy']\n",
    "                    combined_df.loc[combined_index, 'intensity_not_norm'] = row['intensity_not_norm']\n",
    "\n",
    "# Assign a name to the big dataframe before saving it\n",
    "big_dataframe = combined_df\n",
    "\n",
    "# Save the big dataframe\n",
    "#big_dataframe.to_csv(os.path.join(home_dir, \"final_combined_df.csv\"), index=False)\n",
    "\n",
    "print(\"The final_combined_dataframe.csv has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of combinations of experimental_condition and cell_condition\n",
    "total_combinations = combined_df.groupby(['technology', 'ground_truth']).size().reset_index(name='counts')\n",
    "\n",
    "# Calculate the number of combinations that have a track_ID (i.e., not NaN)\n",
    "combinations_with_track_ID = combined_df.dropna(subset=['track_ID']).groupby(['technology', 'ground_truth']).size().reset_index(name='counts')\n",
    "\n",
    "# Calculate the number of combinations with a track_ID and group by prediction\n",
    "combinations_with_track_ID_and_prediction = combined_df.dropna(subset=['track_ID']).groupby(['technology', 'ground_truth', 'prediction']).size().reset_index(name='counts')\n",
    "\n",
    "# Calculate the number of combinations that do not have a track_ID (i.e., NaN) and group by prediction\n",
    "combinations_with_na_track_ID_and_prediction = combined_df.groupby(['technology', 'ground_truth', 'prediction']).size().reset_index(name='counts')\n",
    "\n",
    "# Print the total number of combinations\n",
    "print(f\"Total number of experimental_condition and cell_condition combinations:\")\n",
    "print(total_combinations)\n",
    "\n",
    "# Print the number of combinations that have a track_ID\n",
    "print(f\"\\nNumber of combinations with track_ID (non-NaN):\")\n",
    "print(combinations_with_track_ID)\n",
    "\n",
    "# Print the number of combinations that do not have a track_ID grouped by prediction\n",
    "print(f\"\\nNumber of combinations with NaN track_ID grouped by prediction:\")\n",
    "print(combinations_with_na_track_ID_and_prediction)\n",
    "\n",
    "# Print the number of combinations with track_ID grouped by prediction\n",
    "print(f\"\\nNumber of combinations with track_ID (non-NaN) grouped by prediction:\")\n",
    "print(combinations_with_track_ID_and_prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "isolatrix_livecell_tp = combined_df[(combined_df['technology'] == 'Isolatrix') & \n",
    "                                    (combined_df['cell_condition'] == 'LiveCell') & \n",
    "                                    (combined_df['prediction'] == 0)][['track_ID', 'subdirectory_name']].dropna(subset=['track_ID'])\n",
    "\n",
    "# Print the filtered rows\n",
    "print(isolatrix_livecell_tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "isolatrix_livecell_tp = combined_df[(combined_df['technology'] == 'Isolatrix') & \n",
    "                                    (combined_df['cell_condition'] == 'NCC') & \n",
    "                                    (combined_df['prediction'] == 0)][['track_ID', 'subdirectory_name']].dropna(subset=['track_ID'])\n",
    "\n",
    "# Print the filtered rows\n",
    "print(isolatrix_livecell_tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot TP and FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "from cliffs_delta import cliffs_delta\n",
    "\n",
    "def create_violinplot_with_boxplot(data, xticklabels, title, output_directory=None):\n",
    "    \"\"\"\n",
    "    Create a violin plot with embedded boxplots for multiple lists of values,\n",
    "    and display the results of a Mann-Whitney U test and Cliff's Delta for effect size.\n",
    "    \"\"\"\n",
    "    # Check that the data list is not empty\n",
    "    if not data or all(len(d) == 0 for d in data):\n",
    "        print(\"No data available for plotting.\")\n",
    "        return\n",
    "\n",
    "    # Remove empty lists and their corresponding labels\n",
    "    filtered_data = [d for d in data if len(d) > 0]\n",
    "    filtered_labels = [f\"{xticklabels[i]} (n={len(data[i])})\" for i in range(len(data)) if len(data[i]) > 0]\n",
    "    \n",
    "    if not filtered_data:\n",
    "        print(\"No data available for plotting after filtering empty lists.\")\n",
    "        return\n",
    "\n",
    "    # Colors for the violins (shades of blue)\n",
    "    colors = ['#5A79A5', '#334F73']  # Two different shades of blue\n",
    "    \n",
    "    # Perform Mann-Whitney U Test\n",
    "    u_statistic, p_value = mannwhitneyu(filtered_data[0], filtered_data[1], alternative='two-sided')\n",
    "    \n",
    "    # Calculate Cliff's Delta for effect size\n",
    "    delta, magnitude = cliffs_delta(filtered_data[0], filtered_data[1])\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create violin plot\n",
    "    parts = ax.violinplot(filtered_data, showmeans=False, showmedians=True, showextrema=True)\n",
    "    \n",
    "    # Customize the violin plot colors\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(colors[i % len(colors)])  # Assign colors cyclically if more violins than colors\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Add boxplots\n",
    "    boxprops = dict(linestyle='-', linewidth=2, color='black')\n",
    "    medianprops = dict(linestyle='-', linewidth=2.5, color='red')\n",
    "    flierprops = dict(marker='o', color='black', markersize=3, markerfacecolor='black')\n",
    "    ax.boxplot(filtered_data, positions=np.arange(1, len(filtered_data) + 1), widths=0.1,\n",
    "               boxprops=boxprops, medianprops=medianprops, showmeans=False, flierprops=flierprops)\n",
    "    \n",
    "    # Set axis labels and title\n",
    "    ax.set_xticks(np.arange(1, len(filtered_labels) + 1))\n",
    "    ax.set_xticklabels(filtered_labels, rotation=0, ha='center')\n",
    "    ax.set_ylabel(title.capitalize())\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Annotate the x-axis to show which category each violin belongs to\n",
    "    ax.text(0.25, -0.1, 'True Positive', ha='center', va='center', transform=ax.transAxes, fontsize=12, fontweight='bold')\n",
    "    ax.text(0.75, -0.1, 'False Negative', ha='center', va='center', transform=ax.transAxes, fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Add grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Display the statistical test results on the plot in the top left corner\n",
    "    ax.text(0.02, 0.99, f'Wilcoxon: p-value={p_value:.3f}  Effect Size: {delta:.2f} ({magnitude})',\n",
    "            transform=ax.transAxes, fontsize=12, verticalalignment='top')#, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    # Adjust layout to make room for the legend\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust rect to leave space for the legend\n",
    "    \n",
    "    # Save the plot if an output directory is provided\n",
    "    if output_directory:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_directory, f'{title}_violin.svg'), format = \"svg\")\n",
    "        plt.savefig(os.path.join(output_directory, f'{title}_violin.png'))\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_isolatrix_tp_fn(combined_df, feature_name, output_directory=None):\n",
    "    \"\"\"\n",
    "    Plots a violin plot with embedded box plots for Isolatrix True Positives (TP) and False Negatives (FN).\n",
    "\n",
    "    Parameters:\n",
    "    - combined_df: The combined dataframe containing the data.\n",
    "    - feature_name: The column name of the feature to be plotted.\n",
    "    - output_directory: Directory to save the plot (default is None).\n",
    "    \"\"\"\n",
    "    isolatrix_tp = combined_df[(combined_df['technology'] == 'Isolatrix') & \n",
    "                               (combined_df['cell_condition'] == 'LiveCell') & \n",
    "                               (combined_df['prediction'] == 1)][feature_name].dropna().tolist()\n",
    "\n",
    "    isolatrix_fn = combined_df[(combined_df['technology'] == 'Isolatrix') & \n",
    "                               (combined_df['cell_condition'] == 'LiveCell') & \n",
    "                               (combined_df['prediction'] == 0)][feature_name].dropna().tolist()\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    data = [isolatrix_tp, isolatrix_fn] \n",
    "    xticklabels = ['True Positive', 'False Negative']\n",
    "\n",
    "    # Create the plot\n",
    "    if feature_name == \"circularity\": feature_name = \"Circularity\"\n",
    "    if feature_name == \"intensity\": feature_name = \"Normalized Intensity\"\n",
    "    if feature_name == \"area\": feature_name = \"Area (n Pixels)\"\n",
    "    if feature_name == \"energy\": feature_name = \"Textual Homogeneity\"\n",
    "    create_violinplot_with_boxplot(data, xticklabels, feature_name, output_directory)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_isolatrix_tp_fn(combined_df, \"circularity\", output_directory=\"/projects/steiflab/scratch/leli/A138856A/plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_isolatrix_tp_fn(combined_df, \"intensity\", output_directory=\"/projects/steiflab/scratch/leli/A138856A/plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_isolatrix_tp_fn(combined_df, \"intensity_not_norm\", output_directory=\"/projects/steiflab/scratch/leli/A138856A/plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_isolatrix_tp_fn(combined_df, \"area\", output_directory=\"/projects/steiflab/scratch/leli/A138856A/plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_isolatrix_tp_fn(combined_df, \"energy\", output_directory=\"/projects/steiflab/scratch/leli/A138856A/plots/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot class 0 and class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "from cliffs_delta import cliffs_delta\n",
    "\n",
    "def add_stat_annotation(ax, p_value, effect_size, magnitude, x1, x2, y, h, col):\n",
    "    \"\"\"\n",
    "    Annotates the plot with the p-value and effect size.\n",
    "\n",
    "    Parameters:\n",
    "    - ax: Matplotlib axis object.\n",
    "    - p_value: The p-value to annotate.\n",
    "    - effect_size: The effect size to annotate.\n",
    "    - x1, x2: The positions on the x-axis for the two groups.\n",
    "    - y: The y position to place the annotation.\n",
    "    - h: The height of the annotation.\n",
    "    - col: The color of the annotation.\n",
    "    \"\"\"\n",
    "    text = f\"p={p_value:.4f}, effect size={effect_size:.2f} ({magnitude})\"\n",
    "    if p_value < 0.0001: text = f\"p < 0.0001, effect size={effect_size:.2f} ({magnitude})\"\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + h, y + h, y], lw=1.5, c=col)\n",
    "    ax.text((x1 + x2) * 0.5, y + h, text, ha='center', va='bottom', color=col, fontsize=14)\n",
    "\n",
    "def create_violinplot_with_boxplot(data, xticklabels, title, output_directory=None):\n",
    "    \"\"\"\n",
    "    Create a violin plot with embedded boxplots for multiple lists of values.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of lists containing numerical values to plot.\n",
    "    - xticklabels: Labels for the x-axis corresponding to each list in data.\n",
    "    - title: Title of the plot.\n",
    "    - output_directory: Directory to save the plot (default is None).\n",
    "    \"\"\"\n",
    "    # Check that the data list is not empty\n",
    "    if not data or all(len(d) == 0 for d in data):\n",
    "        print(\"No data available for plotting.\")\n",
    "        return\n",
    "\n",
    "    # Remove empty lists and their corresponding labels\n",
    "    filtered_data = [d for d in data if len(d) > 0]\n",
    "    filtered_labels = [f\"{xticklabels[i]} (n={len(data[i])})\" for i in range(len(data)) if len(data[i]) > 0]\n",
    "    \n",
    "    if not filtered_data:\n",
    "        print(\"No data available for plotting after filtering empty lists.\")\n",
    "        return\n",
    "\n",
    "    # Colors for the violins\n",
    "    colors = ['#5A79A5', '#5A79A5', '#F3E197', '#F3E197']  \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create violin plot\n",
    "    parts = ax.violinplot(filtered_data, showmeans=False, showmedians=True, showextrema=True)\n",
    "    \n",
    "    # Customize the violin plot colors\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(colors[i % len(colors)])  # Assign colors cyclically if more violins than colors\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(0.7)\n",
    "    \n",
    "    # Add boxplots\n",
    "    boxprops = dict(linestyle='-', linewidth=2, color='black')\n",
    "    medianprops = dict(linestyle='-', linewidth=2.5, color='red')\n",
    "    flierprops = dict(marker='o', color='black', markersize=3, markerfacecolor='black')\n",
    "    ax.boxplot(filtered_data, positions=np.arange(1, len(filtered_data) + 1), widths=0.1,\n",
    "               boxprops=boxprops, medianprops=medianprops, showmeans=False, flierprops=flierprops)\n",
    "    \n",
    "    # Set axis labels and title\n",
    "    ax.set_xticks(np.arange(1, len(filtered_labels) + 1))\n",
    "    ax.set_xticklabels(filtered_labels, rotation=0, ha='center')\n",
    "    ax.set_ylabel(title.capitalize())\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Annotate the x-axis to show which technology each pair belongs to\n",
    "    ax.text(0.25, -0.1, 'Isolatrix', ha='center', va='center', transform=ax.transAxes, fontsize=12, fontweight='bold')\n",
    "    ax.text(0.75, -0.1, 'CellenONE', ha='center', va='center', transform=ax.transAxes, fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Add grid for better readability\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Statistical analysis and annotations\n",
    "    pairs = [(0, 1), (2, 3)]\n",
    "    y_max = max(max(data[0]), max(data[1]), max(data[2]), max(data[3]))\n",
    "    y_min = min(min(data[0]), min(data[1]), min(data[2]), min(data[3]))\n",
    "    h = (y_max - y_min) * 0.05  # Height of the bracket\n",
    "    for (x1, x2) in pairs:\n",
    "        # Perform Mann-Whitney U test (Wilcoxon rank-sum test)\n",
    "        if len(data[x1]) > 0 and len(data[x2]) > 0:\n",
    "            _, p_value = mannwhitneyu(data[x1], data[x2], alternative='two-sided')\n",
    "            # Calculate effect size (Cliff's delta)\n",
    "            effect_size, magnitude = cliffs_delta(data[x1], data[x2])\n",
    "            # Annotate the plot\n",
    "            add_stat_annotation(ax, p_value, effect_size, magnitude, x1 + 1, x2 + 1, y_max + h, h, 'black')\n",
    "            y_max += h * 2  # Update y_max for next annotation\n",
    "\n",
    "    # Adjust layout to make room for the legend\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust rect to leave space for the legend\n",
    "    \n",
    "    # Save the plot if an output directory is provided\n",
    "    if output_directory:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_directory, f'{title}_Iso_cell_comparison_violin.png'), dpi=300)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_violin(home_directory, feature_name, output_directory=None):\n",
    "    \"\"\"\n",
    "    Plots a violin plot with embedded box plots for a specified feature across different categories:\n",
    "    Isolatrix isolated, Isolatrix discarded, CellenONE isolated, CellenONE discarded.\n",
    "\n",
    "    Parameters:\n",
    "    - home_directory: The home directory containing the folders.\n",
    "    - feature_name: The column name of the feature to be plotted.\n",
    "    - output_directory: Directory to save the plot (default is None).\n",
    "    \"\"\"\n",
    "    isolatrix_isolated = []\n",
    "    isolatrix_discarded = []\n",
    "    cellenone_isolated = []\n",
    "    cellenone_discarded = []\n",
    "    \n",
    "    empty_datasets = {\"Isolatrix Isolated\": 0, \"Isolatrix Discarded\": 0, \"CellenONE Isolated\": 0, \"CellenONE Discarded\": 0}\n",
    "    \n",
    "    # Traverse the home directory\n",
    "    for root, dirs, files in os.walk(home_directory):\n",
    "        for file in files:\n",
    "            if file in [\"features_p1.csv\", \"features_p0.csv\", \"features_p2.csv\", \"features.csv\"]:\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                df = pd.read_csv(file_path)\n",
    "                if feature_name not in df.columns:\n",
    "                    print(f\"Feature '{feature_name}' not found in file: {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                if \"dropRun\" in root:\n",
    "                    if file == \"features_p1.csv\":\n",
    "                        if df[feature_name].dropna().empty:\n",
    "                            empty_datasets[\"Isolatrix Isolated\"] += 1\n",
    "                            print(f\"No data in 'Isolatrix Isolated' for file: {file_path}\")\n",
    "                        else:\n",
    "                            isolatrix_isolated.extend(df[feature_name].dropna().tolist())\n",
    "                            print(f\"isolatrix isolated: {len(isolatrix_isolated)}\")\n",
    "                    else:\n",
    "                        if df[feature_name].dropna().empty:\n",
    "                            empty_datasets[\"Isolatrix Discarded\"] += 1\n",
    "                            print(f\"No data in 'Isolatrix Discarded' for file: {file_path}\")\n",
    "                        else:\n",
    "                            isolatrix_discarded.extend(df[feature_name].dropna().tolist())\n",
    "                            print(f\"isolatrix discarded: {len(isolatrix_discarded)}\")\n",
    "                else:\n",
    "                    if file == \"features.csv\":\n",
    "                        df_isolated = df[df['label'] == 'isolated']\n",
    "                        df_discarded = df[df['label'] == 'discarded']\n",
    "                        cellenone_isolated.extend(df_isolated[feature_name].dropna().tolist())\n",
    "                        cellenone_discarded.extend(df_discarded[feature_name].dropna().tolist())\n",
    "\n",
    "    # Log empty datasets\n",
    "    for category, count in empty_datasets.items():\n",
    "        if count > 0:\n",
    "            print(f\"Empty dataset found in {category}: {count} files with no data\")\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    data = [\n",
    "        isolatrix_isolated,\n",
    "        isolatrix_discarded,\n",
    "        cellenone_isolated,\n",
    "        cellenone_discarded\n",
    "    ]\n",
    "    xticklabels = ['Class 1', 'Class 0', 'Isolated', 'Discarded']\n",
    "\n",
    "    if feature_name == \"energy\": feature_name = \"Textual Homogeneity\"\n",
    "    if feature_name == \"circularity\": feature_name = \"Circularity\"\n",
    "    if feature_name == \"area\": feature_name = \"Area\"\n",
    "    if feature_name == \"intensity\": feature_name = \"Normalized Intensity\"\n",
    "\n",
    "    # Create the plot\n",
    "    create_violinplot_with_boxplot(data, xticklabels, feature_name, output_directory)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "data = plot_feature_violin(\"/projects/steiflab/scratch/leli/A138856A\", \"area\", output_directory = \"/projects/steiflab/scratch/leli/A138856A/plots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_feature_violin(\"/projects/steiflab/scratch/leli/A138856A\", \"circularity\", \"/projects/steiflab/scratch/leli/A138856A/plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_feature_violin(\"/projects/steiflab/scratch/leli/A138856A\", \"intensity\", \"/projects/steiflab/scratch/leli/A138856A/plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = plot_feature_violin(\"/projects/steiflab/scratch/leli/A138856A\", \"energy\", \"/projects/steiflab/scratch/leli/A138856A/plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile to 1 Fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Define the desired size in centimeters\n",
    "width_cm = 17.1  # Double column width as per the requirement\n",
    "height_cm = 14  # Maximum allowed height\n",
    "\n",
    "# Convert to inches\n",
    "width_inch = width_cm / 2.54\n",
    "height_inch = height_cm / 2.54\n",
    "\n",
    "# Create the figure and the grid spec\n",
    "fig = plt.figure(figsize=(width_inch, height_inch), dpi=600)\n",
    "gs = gridspec.GridSpec(2, 2, figure=fig, height_ratios=[1, 1])\n",
    "\n",
    "# Reduce the spacing between subplots\n",
    "#gs.update(wspace=0.1, hspace=0.1)\n",
    "\n",
    "# Load the images\n",
    "metrics_img = plt.imread('/projects/steiflab/scratch/leli/A138856A/metrics_barplot.png')\n",
    "circularity_img = plt.imread('/projects/steiflab/scratch/leli/A138856A/Circularity_violin.png')\n",
    "energy_img = plt.imread('/projects/steiflab/scratch/leli/A138856A/GLCM Energy_violin.png')\n",
    "intensity_img = plt.imread('/projects/steiflab/scratch/leli/A138856A/Intensity Z-score_violin.png')\n",
    "\n",
    "# Plot the metrics bar plot on the left column, spanning all three rows\n",
    "ax4 = fig.add_subplot(gs[0, 0])\n",
    "ax4.imshow(metrics_img, aspect='auto', extent=ax4.get_xlim() + ax4.get_ylim())\n",
    "ax4.axis('off')\n",
    "ax4.set_title('A', loc='left', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot the circularity violin plot on the top right\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax1.imshow(circularity_img, aspect='auto', extent=ax1.get_xlim() + ax1.get_ylim())\n",
    "ax1.axis('off')\n",
    "ax1.set_title('B', loc='left', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot the energy violin plot on the bottom left\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.imshow(energy_img, aspect='auto', extent=ax2.get_xlim() + ax2.get_ylim())\n",
    "ax2.axis('off')\n",
    "ax2.set_title('C', loc='left', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot the intensity violin plot on the bottom right\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.imshow(intensity_img, aspect='auto', extent=ax3.get_xlim() + ax3.get_ylim())\n",
    "ax3.axis('off')\n",
    "ax3.set_title('D', loc='left', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Adjust layout and save the final figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('/projects/steiflab/scratch/leli/A138856A/combined_figure.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tifffile import imread\n",
    "import csv\n",
    "import numpy as np \n",
    "# Your home directory path (change this to your actual path)\n",
    "home_dir = \"/projects/steiflab/archive/data/imaging/A138856A/NozzleImages\"\n",
    "\n",
    "well_df = pd.DataFrame()\n",
    "# Traverse through each subdirectory in the home directory\n",
    "for root, dirs, files in os.walk(home_dir):\n",
    "    for file in files:\n",
    "        if file == \"LogFile.csv\":\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Read the logfile.csv file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Group by R and C, then find the row with the maximum Prediction for each group\n",
    "            df_max_pred = df.loc[df.groupby(['R', 'C'])['Prediction'].idxmax()][['R', 'C', 'Prediction']]\n",
    "            well_df = pd.concat([well_df, df_max_pred], ignore_index=True)\n",
    "\n",
    "# After gathering the data, read the new file to merge\n",
    "new_file_path = \"/projects/steiflab/scratch/glchang/other/leon/A138856.tsv\"\n",
    "new_df = []\n",
    "with open(new_file_path, mode='r') as file:\n",
    "    reader = csv.reader(file, delimiter=' ')\n",
    "    \n",
    "    for row in reader:\n",
    "        new_df.append(row)\n",
    "\n",
    "new_df = pd.DataFrame(new_df)\n",
    "new_df.columns = new_df.iloc[0]  # Set the first row as the header\n",
    "new_df = new_df[1:]  # Remove the first row from the data\n",
    "new_df.reset_index(drop=True, inplace=True)  # Reset the index\n",
    "# Convert the necessary columns to integers\n",
    "new_df['row'] = new_df['row'].astype(int)\n",
    "new_df['col'] = new_df['col'].astype(int)\n",
    "new_df['total_mapped_reads'] = new_df['total_mapped_reads'].astype(int)\n",
    "print(np.unique(new_df['experimental_condition']))\n",
    "print(np.unique(new_df['cell_condition']))\n",
    "print(new_df.shape)\n",
    "\n",
    "combination_counts = new_df.groupby(['experimental_condition', 'cell_condition']).size().reset_index(name='counts')\n",
    "combination_counts.sort_values(by='counts', ascending=False, inplace=True)\n",
    "\n",
    "print(combination_counts)\n",
    "\n",
    "\n",
    "# Keep only the 'row', 'col', and 'total_reads' columns\n",
    "new_df = new_df[['row', 'col', 'total_mapped_reads']]\n",
    "\n",
    "# Assuming well_df and new_df are your dataframes\n",
    "\n",
    "# Step 1: Create the 'manual_inspection' column\n",
    "well_df['manual_inspection'] = well_df.apply(\n",
    "    lambda row: 1 if ((row['R'], row['C']) in zip(new_df['row'], new_df['col'])) else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 2: Create the 'mapped_reads' column\n",
    "well_df['mapped_reads'] = well_df.apply(\n",
    "    lambda row: new_df.loc[(new_df['row'] == row['R']) & (new_df['col'] == row['C']), 'total_mapped_reads'].values[0]\n",
    "    if ((row['R'], row['C']) in zip(new_df['row'], new_df['col'])) else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Create the 'max_intensity' column by reading the .tif file\n",
    "def get_max_intensity(r, c):\n",
    "    file_path = f\"/projects/steiflab/archive/data/imaging/A138856A/MicroscopeImages/S0000/C{str(c).zfill(2)}/R{str(r).zfill(2)}_C{str(c).zfill(2)}_0000_00_Cyan.tif\"\n",
    "    if os.path.exists(file_path):\n",
    "        image = imread(file_path)\n",
    "        return image.max()\n",
    "    else:\n",
    "        print(\"The fluorescent image does not exist\")\n",
    "        return None  # Or you can return 0 if you prefer\n",
    "\n",
    "well_df['fluro_intensity'] = well_df.apply(\n",
    "    lambda row: get_max_intensity(row['R'], row['C']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display the updated dataframe\n",
    "\n",
    "well_df.to_csv(\"/projects/steiflab/scratch/leli/A138856A/gt_inspection.csv\", index  = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of singlecells and not singlecells are: {well_df['manual_inspection'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming well_df is already prepared with the new columns\n",
    "\n",
    "# Correlation matrix to see how the columns are related\n",
    "correlation_matrix = well_df[['Prediction', 'manual_inspection', 'mapped_reads', 'fluro_intensity']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Ground Truth Methods')\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming well_df is your dataframe with the necessary columns\n",
    "\n",
    "# Create a violin plot for Prediction vs. max_intensity\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='Prediction', y='fluro_intensity', data=well_df, inner=None, palette=\"muted\")\n",
    "sns.boxplot(x='Prediction', y='fluro_intensity', data=well_df, width=0.1, palette=\"muted\", showcaps=True, showfliers=False, whiskerprops={'linewidth':2}, boxprops={'facecolor':'white', 'edgecolor':'black'}, medianprops={'color':'red', 'linewidth':2})\n",
    "\n",
    "plt.title(\"Violin and Boxplot of fluro_intensity by Prediction\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Max Fluorescent Intensity\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming well_df is your dataframe with the necessary columns\n",
    "\n",
    "# Create a violin plot for manual_inspection vs. max_intensity\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='manual_inspection', y='fluro_intensity', data=well_df, inner=None, palette=\"muted\")\n",
    "sns.boxplot(x='manual_inspection', y='fluro_intensity', data=well_df, width=0.1, palette=\"muted\", showcaps=True, showfliers=False, whiskerprops={'linewidth':2}, boxprops={'facecolor':'white', 'edgecolor':'black'}, medianprops={'color':'red', 'linewidth':2})\n",
    "\n",
    "plt.title(\"Violin and Boxplot of Max fluro_intensity by Manual Inspection\")\n",
    "plt.xlabel(\"Manual Inspection (0 or 1)\")\n",
    "plt.ylabel(\"Max Fluorescent Intensity\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a violin plot for Prediction vs. max_intensity\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='Prediction', y='mapped_reads', data=well_df, inner=None, palette=\"muted\")\n",
    "sns.boxplot(x='Prediction', y='mapped_reads', data=well_df, width=0.1, palette=\"muted\", showcaps=True, showfliers=False, whiskerprops={'linewidth':2}, boxprops={'facecolor':'white', 'edgecolor':'black'}, medianprops={'color':'red', 'linewidth':2})\n",
    "\n",
    "plt.title(\"Violin and Boxplot of mapped_reads by Prediction\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"mapped_reads\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming well_df is your DataFrame with 'Prediction' and 'manual_inspection' columns\n",
    "\n",
    "# Create a count matrix between 'Prediction' and 'manual_inspection'\n",
    "count_matrix = pd.crosstab(well_df['Prediction'], well_df['manual_inspection'])\n",
    "\n",
    "# Display the count matrix\n",
    "print(count_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the relevant columns for UMAP\n",
    "data = well_df[['Prediction', 'manual_inspection', 'max_intensity', 'mapped_reads']]\n",
    "\n",
    "# Apply UMAP to reduce the dimensions to 2D\n",
    "reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "embedding = reducer.fit_transform(data)\n",
    "\n",
    "# Create a DataFrame with UMAP results\n",
    "umap_df = pd.DataFrame(embedding, columns=['UMAP_1', 'UMAP_2'])\n",
    "\n",
    "# Plot the UMAP results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='UMAP_1', y='UMAP_2', data=umap_df)\n",
    "plt.title('UMAP Projection of Four Features')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming well_df is your DataFrame containing the relevant columns\n",
    "\n",
    "# Select the four features\n",
    "features = well_df[['Prediction', 'manual_inspection', 'max_intensity', 'mapped_reads']]\n",
    "\n",
    "# Perform KMeans clustering with 2 clusters\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "well_df['Cluster_2'] = kmeans.fit_predict(features)\n",
    "# Analyze the range for each cluster\n",
    "# Cluster 0 analysis\n",
    "# Cluster 0 analysis\n",
    "cluster_0 = well_df[well_df['Cluster_2'] == 0]\n",
    "print(\"Cluster 0 Analysis:\")\n",
    "print(f\"Prediction: min = {cluster_0['Prediction'].min()}, max = {cluster_0['Prediction'].max()}, mean = {cluster_0['Prediction'].mean()}, median = {cluster_0['Prediction'].median()}\")\n",
    "print(f\"Manual Inspection: min = {cluster_0['manual_inspection'].min()}, max = {cluster_0['manual_inspection'].max()}, mean = {cluster_0['manual_inspection'].mean()}, median = {cluster_0['manual_inspection'].median()}\")\n",
    "print(f\"Max Intensity: min = {cluster_0['max_intensity'].min()}, max = {cluster_0['max_intensity'].max()}, mean = {cluster_0['max_intensity'].mean()}, median = {cluster_0['max_intensity'].median()}\")\n",
    "print(f\"Mapped Reads: min = {cluster_0['mapped_reads'].min()}, max = {cluster_0['mapped_reads'].max()}, mean = {cluster_0['mapped_reads'].mean()}, median = {cluster_0['mapped_reads'].median()}\")\n",
    "\n",
    "# Cluster 1 analysis\n",
    "cluster_1 = well_df[well_df['Cluster_2'] == 1]\n",
    "print(\"\\nCluster 1 Analysis:\")\n",
    "print(f\"Prediction: min = {cluster_1['Prediction'].min()}, max = {cluster_1['Prediction'].max()}, mean = {cluster_1['Prediction'].mean()}, median = {cluster_1['Prediction'].median()}\")\n",
    "print(f\"Manual Inspection: min = {cluster_1['manual_inspection'].min()}, max = {cluster_1['manual_inspection'].max()}, mean = {cluster_1['manual_inspection'].mean()}, median = {cluster_1['manual_inspection'].median()}\")\n",
    "print(f\"Max Intensity: min = {cluster_1['max_intensity'].min()}, max = {cluster_1['max_intensity'].max()}, mean = {cluster_1['max_intensity'].mean()}, median = {cluster_1['max_intensity'].median()}\")\n",
    "print(f\"Mapped Reads: min = {cluster_1['mapped_reads'].min()}, max = {cluster_1['mapped_reads'].max()}, mean = {cluster_1['mapped_reads'].mean()}, median = {cluster_1['mapped_reads'].median()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groun truth analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tifffile import imread\n",
    "\n",
    "# Load the final_combined_df\n",
    "final_combined_df_path = \"/projects/steiflab/scratch/leli/A138856A/final_combined_df.csv\"\n",
    "final_combined_df = pd.read_csv(final_combined_df_path)\n",
    "\n",
    "# Load the new_df data\n",
    "new_file_path = \"/projects/steiflab/scratch/glchang/other/leon/A138856.tsv\"\n",
    "new_df = []\n",
    "with open(new_file_path, mode='r') as file:\n",
    "    reader = csv.reader(file, delimiter=' ')\n",
    "    \n",
    "    for row in reader:\n",
    "        new_df.append(row)\n",
    "\n",
    "new_df = pd.DataFrame(new_df)\n",
    "new_df.columns = new_df.iloc[0]  # Set the first row as the header\n",
    "new_df = new_df[1:]  # Remove the first row from the data\n",
    "new_df.reset_index(drop=True, inplace=True)  # Reset the index\n",
    "\n",
    "# Convert the necessary columns to integers\n",
    "new_df['row'] = new_df['row'].astype(int)\n",
    "new_df['col'] = new_df['col'].astype(int)\n",
    "new_df['total_mapped_reads'] = new_df['total_mapped_reads'].astype(int)\n",
    "\n",
    "# Create 'total_mapped_reads' column in final_combined_df\n",
    "final_combined_df['total_mapped_reads'] = final_combined_df.apply(\n",
    "    lambda row: new_df.loc[(new_df['row'] == row['row']) & (new_df['col'] == row['col']), 'total_mapped_reads'].values[0]\n",
    "    if ((row['row'], row['col']) in zip(new_df['row'], new_df['col'])) else pd.NA,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create 'fluro_intensity' column in final_combined_df by reading the .tif file\n",
    "def get_max_intensity(r, c):\n",
    "    file_path = f\"/projects/steiflab/archive/data/imaging/A138856A/MicroscopeImages/S0000/C{str(c).zfill(2)}/R{str(r).zfill(2)}_C{str(c).zfill(2)}_0000_00_Cyan.tif\"\n",
    "    if os.path.exists(file_path):\n",
    "        image = imread(file_path)\n",
    "        return image.max()\n",
    "    else:\n",
    "        print(f\"The fluorescent image for R{r}, C{c} does not exist\")\n",
    "        return pd.NA  # Or you can return 0 if you prefer\n",
    "\n",
    "final_combined_df['fluro_intensity'] = final_combined_df.apply(\n",
    "    lambda row: get_max_intensity(row['row'], row['col']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Identify rows without 'total_mapped_reads' or 'fluro_intensity'\n",
    "missing_mapped_reads = final_combined_df[final_combined_df['total_mapped_reads'].isna()]\n",
    "missing_fluro_intensity = final_combined_df[final_combined_df['fluro_intensity'].isna()]\n",
    "\n",
    "# Print out the rows missing 'total_mapped_reads'\n",
    "if not missing_mapped_reads.empty:\n",
    "    print(\"Rows without 'total_mapped_reads':\")\n",
    "    print(missing_mapped_reads[['row', 'col']])\n",
    "\n",
    "# Print out the rows missing 'fluro_intensity'\n",
    "if not missing_fluro_intensity.empty:\n",
    "    print(\"Rows without 'fluro_intensity':\")\n",
    "    print(missing_fluro_intensity[['row', 'col']])\n",
    "\n",
    "# Save the updated final_combined_df\n",
    "updated_final_combined_df_path = \"/projects/steiflab/scratch/leli/A138856A/updated_final_combined_df.csv\"\n",
    "final_combined_df.to_csv(updated_final_combined_df_path, index=False)\n",
    "\n",
    "print(\"The updated final_combined_df.csv has been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df_path = \"/projects/steiflab/scratch/leli/A138856A/updated_final_combined_df.csv\"\n",
    "final_combined_df = pd.read_csv(final_combined_df_path)\n",
    "\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "\n",
    "def get_95_percentile_intensity(r, c):\n",
    "    file_path = f\"/projects/steiflab/archive/data/imaging/A138856A/MicroscopeImages/S0000/C{str(c).zfill(2)}/R{str(r).zfill(2)}_C{str(c).zfill(2)}_0000_00_Cyan.tif\"\n",
    "    if os.path.exists(file_path):\n",
    "        image = imread(file_path)\n",
    "        # Calculate the 95th percentile value\n",
    "        return np.percentile(image, 95)\n",
    "    else:\n",
    "        print(f\"The fluorescent image for R{r}, C{c} does not exist\")\n",
    "        return pd.NA  # Or you can return 0 if you prefer\n",
    "   \n",
    "final_combined_df['fluro_intensity_95'] = final_combined_df.apply(\n",
    "    lambda row: get_95_percentile_intensity(row['row'], row['col']),\n",
    "    axis=1\n",
    ")\n",
    "#final_combined_df.to_csv(updated_final_combined_df_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_combined_df['fluro_intensity'].mean() )\n",
    "\n",
    "print(final_combined_df['fluro_intensity_95'].mean() )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataframe\n",
    "final_combined_df_path = \"/projects/steiflab/scratch/leli/A138856A/updated_final_combined_df.csv\"\n",
    "final_combined_df = pd.read_csv(final_combined_df_path)\n",
    "\n",
    "# Ensure the relevant columns are numeric\n",
    "final_combined_df['prediction'] = pd.to_numeric(final_combined_df['prediction'], errors='coerce')\n",
    "final_combined_df['ground_truth'] = pd.to_numeric(final_combined_df['ground_truth'], errors='coerce')\n",
    "final_combined_df['total_mapped_reads'] = pd.to_numeric(final_combined_df['total_mapped_reads'], errors='coerce')\n",
    "final_combined_df['fluro_intensity_95'] = pd.to_numeric(final_combined_df['fluro_intensity_95'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in any of the columns\n",
    "corr_df = final_combined_df[['prediction', 'ground_truth', 'total_mapped_reads', 'fluro_intensity_95']]#.dropna()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = corr_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", vmin=-1, vmax=1, square=True, linewidths=0.5, cbar_kws={\"shrink\": .75})\n",
    "\n",
    "# Set the title\n",
    "plt.title('Correlation Matrix of Prediction, Ground Truth, Total Mapped Reads, and Fluro Intensity 95 percentile', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "# plt.savefig(\"/projects/steiflab/scratch/leli/A138856A/correlation_matrix_heatmap.png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df.to_csv(\"/projects/steiflab/scratch/leli/A138856A/final_combined_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "from cliffs_delta import cliffs_delta\n",
    "\n",
    "def create_violin_boxplot_with_stats(df, x, y, output_directory):\n",
    "    # Extract the data for the two categories\n",
    "    categories = df[x].unique()\n",
    "    if len(categories) != 2:\n",
    "        print(f\"Skipping plot for {x} vs {y}: requires exactly 2 categories.\")\n",
    "        return\n",
    "    \n",
    "    category1 = df[df[x] == categories[0]][y].dropna()\n",
    "    category2 = df[df[x] == categories[1]][y].dropna()\n",
    "\n",
    "    # Perform Mann-Whitney U test\n",
    "    stat, p_value = mannwhitneyu(category1, category2)\n",
    "    \n",
    "    # Calculate effect size (Cliff's Delta)\n",
    "    cliffs_delta_value, magnitude = cliffs_delta(category1, category2)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(x=x, y=y, data=df, inner=None, palette=\"muted\")\n",
    "    sns.boxplot(x=x, y=y, data=df, width=0.1, palette=\"muted\", showcaps=True, showfliers=False, whiskerprops={'linewidth':2}, boxprops={'facecolor':'white', 'edgecolor':'black'}, medianprops={'color':'red', 'linewidth':2})\n",
    "    \n",
    "    # Add the number of samples to the x-axis labels\n",
    "    xticklabels = [f\"{cat} (n={len(df[df[x] == cat])})\" for cat in categories]\n",
    "    plt.xticks(ticks=[0, 1], labels=xticklabels)\n",
    "    \n",
    "    # Add statistical results in a text box\n",
    "    stats_text = f\"Mann-Whitney U p-value: {p_value:.4f}\\nCliff's Delta: {cliffs_delta_value:.2f} ({magnitude})\"\n",
    "    plt.gca().text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=12,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='lightgrey'))\n",
    "    \n",
    "    # Set the title and labels\n",
    "    plt.title(f\"Violin and Boxplot of {y} by {x}\", fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save the plot\n",
    "    # plt.savefig(f\"{output_directory}/violin_boxplot_{x}_vs_{y}.png\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Directory to save plots\n",
    "output_directory = \"/projects/steiflab/scratch/leli/A138856A/violin_plots_with_stats\"\n",
    "\n",
    "# Create and save violin plots for all combinations\n",
    "categorical_vars = ['ground_truth', 'prediction']\n",
    "numerical_vars = ['total_mapped_reads', 'fluro_intensity']\n",
    "\n",
    "for cat_var in categorical_vars:\n",
    "    for num_var in numerical_vars:\n",
    "        create_violin_boxplot_with_stats(final_combined_df, x=cat_var, y=num_var, output_directory=output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(final_combined_df_path)\n",
    "\n",
    "# Features and target\n",
    "X = df[['prediction', 'total_mapped_reads', 'fluro_intensity']]\n",
    "y = df['ground_truth']\n",
    "\n",
    "# Split the data into train and test sets, ensuring a balanced class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# Perform cross-validation on training data\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "# Logistic Regression\n",
    "logreg_scores = cross_validate(logreg, X_train_scaled, y_train, cv=5, scoring=scoring)\n",
    "# Random Forest\n",
    "rf_scores = cross_validate(rf, X_train_scaled, y_train, cv=5, scoring=scoring)\n",
    "# Dummy Classifier\n",
    "dummy_scores = cross_validate(dummy, X_train_scaled, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "# Train the models on the training data\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "dummy.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "models = {'Logistic Regression': logreg, 'Random Forest': rf, 'Dummy Classifier': dummy}\n",
    "best_f1_score = 0\n",
    "best_model = None\n",
    "best_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Performance on Test Set:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1]):.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_model = model\n",
    "        best_name = name\n",
    "\n",
    "# Print the cross-validation results\n",
    "print(\"\\nCross-Validation Performance:\")\n",
    "for name, scores in zip(['Logistic Regression', 'Random Forest', 'Dummy Classifier'], [logreg_scores, rf_scores, dummy_scores]):\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric in scoring:\n",
    "        print(f\"{metric.capitalize()}: {np.mean(scores['test_' + metric]):.4f}\")\n",
    "\n",
    "# Feature Importance for the best model\n",
    "if best_name != \"Dummy Classifier\":\n",
    "    print(f\"\\nBest Model: {best_name} with F1 Score: {best_f1_score:.4f}\")\n",
    "    if best_name == \"Random Forest\":\n",
    "        importance = best_model.feature_importances_\n",
    "    elif best_name == \"Logistic Regression\":\n",
    "        importance = np.abs(best_model.coef_[0])\n",
    "    else:\n",
    "        importance = None\n",
    "    \n",
    "    if importance is not None:\n",
    "        feature_names = X.columns\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "        importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "        print(\"\\nFeature Importances:\\n\", importance_df)\n",
    "else:\n",
    "    print(\"\\nBest Model is the Dummy Classifier, no feature importance to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"/projects/steiflab/scratch/leli/A138856A/final_combined_df.csv\")\n",
    "\n",
    "# Filter to only include Isolatrix technology\n",
    "isolatrix_df = df[df['technology'].str.contains('isolatrix', case=False)]\n",
    "\n",
    "# Identify Isolatrix TP and FN\n",
    "# TP: Ground Truth = 1, Prediction = 1\n",
    "tp_df = isolatrix_df[(isolatrix_df['ground_truth'] == 1) &\n",
    "                     (isolatrix_df['prediction'] == 1)]\n",
    "\n",
    "# FN: Ground Truth = 1, Prediction = 0\n",
    "fn_df = isolatrix_df[(isolatrix_df['ground_truth'] == 1) &\n",
    "                     (isolatrix_df['prediction'] == 0)]\n",
    "\n",
    "# Combine the TP and FN data for plotting\n",
    "tp_fn_data = pd.DataFrame({\n",
    "    'Condition': ['True Positive (TP)'] * len(tp_df) + ['False Negative (FN)'] * len(fn_df),\n",
    "    'Total Mapped Reads': pd.concat([tp_df['total_mapped_reads'], fn_df['total_mapped_reads']])\n",
    "})\n",
    "\n",
    "# Create the violin plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='Condition', y='Total Mapped Reads', data=tp_fn_data, palette=['#5A79A5', '#9EB3D6'])\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Violin Plot of Total Mapped Reads: TP vs FN for Isolatrix\")\n",
    "plt.xlabel(\"Condition\")\n",
    "plt.ylabel(\"Total Mapped Reads\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GT Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt= pd.read_csv('/projects/steiflab/archive/data/wgs/single_cell/internal/A138856/merge/metadata.tsv', delimiter = '\\t')\n",
    "gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Initialize empty DataFrames to store the results\n",
    "df_max_prediction_1 = pd.DataFrame()\n",
    "df_max_prediction_0 = pd.DataFrame()\n",
    "df_max_prediction_2 = pd.DataFrame()\n",
    "\n",
    "# Your home directory path\n",
    "home_dir = \"/projects/steiflab/archive/data/imaging/A138856A/NozzleImages\"\n",
    "\n",
    "# Traverse through each subdirectory in the home directory\n",
    "for root, dirs, files in os.walk(home_dir):\n",
    "    for file in files:\n",
    "        if file == \"LogFile.csv\":\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Read the logfile.csv file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Group by R and C, then find the row with the maximum Prediction for each group\n",
    "            df_max_pred = df.loc[df.groupby(['R', 'C'])['Prediction'].idxmax()]\n",
    "\n",
    "            # Filter for prediction = 1\n",
    "            df_pred_1 = df_max_pred[df_max_pred['Prediction'] == 1][['R', 'C', 'Prediction']]\n",
    "            df_max_prediction_1 = pd.concat([df_max_prediction_1, df_pred_1], ignore_index=True)\n",
    "            \n",
    "            # Filter for prediction = 0\n",
    "            df_pred_0 = df_max_pred[df_max_pred['Prediction'] == 0][['R', 'C', 'Prediction']]\n",
    "            df_max_prediction_0 = pd.concat([df_max_prediction_0, df_pred_0], ignore_index=True)\n",
    "\n",
    "            # Filter for prediction = 2\n",
    "            df_pred_2 = df_max_pred[df_max_pred['Prediction'] == 2][['R', 'C', 'Prediction']]\n",
    "            df_max_prediction_2 = pd.concat([df_max_prediction_2, df_pred_2], ignore_index=True)\n",
    "\n",
    "# Rename columns in df_max_prediction_1, df_max_prediction_0, and df_max_prediction_2\n",
    "df_max_prediction_1.columns = ['row', 'col', 'prediction']\n",
    "df_max_prediction_0.columns = ['row', 'col', 'prediction']\n",
    "df_max_prediction_2.columns = ['row', 'col', 'prediction']\n",
    "\n",
    "print(f\"Count for prediction classes: {(df_max_prediction_0.shape[0], df_max_prediction_1.shape[0], df_max_prediction_2.shape[0])}\")\n",
    "\n",
    "# Concatenate the dataframes together\n",
    "df_combined_predictions = pd.concat([df_max_prediction_1, df_max_prediction_0, df_max_prediction_2], ignore_index=True)\n",
    "\n",
    "# Detect and resolve duplicates in df_combined_predictions by choosing the maximum prediction\n",
    "df_combined_predictions = df_combined_predictions.groupby(['row', 'col'], as_index=False).agg({'prediction': 'max'})\n",
    "\n",
    "print(f\"Shape of combined predictions: {df_combined_predictions.shape}\")\n",
    "\n",
    "# Step 3: Read the new file (A138856.tsv) and prepare the new_df\n",
    "new_df = pd.read_csv('/projects/steiflab/archive/data/wgs/single_cell/internal/A138856/merge/metadata.tsv', delimiter='\\t')\n",
    "\n",
    "print(f\"Shape of metadata: {new_df.shape}\")\n",
    "non_cellenone_shape = new_df[new_df['experimental_condition'] != \"CellenONE\"].shape\n",
    "print(f\"Shape of metadata with non-CellenONE conditions: {non_cellenone_shape}\")\n",
    "\n",
    "# Merge the combined dataframe with new_df\n",
    "merged_df = pd.merge(new_df, df_combined_predictions, how='left', on=['row', 'col'])\n",
    "print(f\"Shape of merged_df: {merged_df.shape}\")\n",
    "\n",
    "# Group by 'experimental_condition', 'cell_condition', and 'prediction'\n",
    "grouped_counts = merged_df.groupby(['experimental_condition', 'cell_condition', 'prediction']).size().reset_index(name='counts') \n",
    "# Display the grouped counts\n",
    "print(grouped_counts)\n",
    "\n",
    "# Check counts in the merged_df where prediction is NaN\n",
    "na_predictions = merged_df[merged_df['prediction'].isna()]\n",
    "print(f\"Count of rows with NaN predictions: {na_predictions.shape[0]}\")\n",
    "\n",
    "# Group and print counts of missing predictions by 'experimental_condition' and 'cell_condition'\n",
    "missing_counts = na_predictions.groupby(['experimental_condition', 'cell_condition']).size().reset_index(name='counts')\n",
    "print(\"Counts of missing predictions grouped by 'experimental_condition' and 'cell_condition':\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Print rows where 'experimental_condition' should be \"CellenONE\"\n",
    "cellenone_in_merged = merged_df[merged_df['experimental_condition'] == \"CellenONE\"]\n",
    "print(f\"Number of rows with 'CellenONE' in merged_df: {cellenone_in_merged.shape[0]}\")\n",
    "\n",
    "\n",
    "'''# Print rows where 'prediction' is NaN\n",
    "nan_predictions_df = merged_df[~merged_df['prediction'].isna()]\n",
    "print(\"Rows with NaN predictions before handling:\")\n",
    "print(f\"nan predicton is is {nan_predictions_df.shape}\")\n",
    "\n",
    "# Print counts grouped by 'experimental_condition' and 'cell_condition' for NaN predictions\n",
    "nan_counts = nan_predictions_df.groupby(['experimental_condition', 'cell_condition']).size().reset_index(name='counts')\n",
    "print(\"Counts for rows with NaN predictions grouped by 'experimental_condition' and 'cell_condition':\")\n",
    "print(nan_counts)\n",
    "\n",
    "merged_df['experimental_condition'].fillna(\"Iso\", inplace=True)  # Default to prediction = 0 if NaN\n",
    "merged_df['cell_condition'].fillna(\"Cell NCC\", inplace=True)  # Default to prediction = 0 if NaN\n",
    "\n",
    "merged_df = merged_df[['row', 'col', 'prediction', 'experimental_condition', 'cell_condition']]\n",
    "isolatrix_df = merged_df[merged_df['prediction'] != 2]\n",
    "isolatrix_df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming merged_df is your dataframe\n",
    "# Sort the dataframe based on 'row' and 'col'\n",
    "merged_df_sorted = merged_df.sort_values(by=['row', 'col']).reset_index(drop=True)\n",
    "\n",
    "# Check for missing entries\n",
    "max_row = merged_df_sorted['row'].max()\n",
    "max_col = merged_df_sorted['col'].max()\n",
    "print(f\"The max row and col are {(merged_df_sorted['row'].min(), merged_df_sorted['col'].min())}\")\n",
    "print(f\"The max row and col are {(max_row, max_col)}\")\n",
    "# Generate all possible (row, col) combinations\n",
    "all_combinations = pd.MultiIndex.from_product([range(1, max_row + 1), range(1, max_col + 1)], names=['row', 'col'])\n",
    "\n",
    "# Convert the sorted dataframe to a MultiIndex for easy comparison\n",
    "sorted_index = pd.MultiIndex.from_frame(merged_df_sorted[['row', 'col']])\n",
    "\n",
    "# Find missing entries\n",
    "missing_entries = all_combinations.difference(sorted_index)\n",
    "\n",
    "# Convert missing entries to a DataFrame for easier manipulation\n",
    "missing_entries_df = missing_entries.to_frame(index=False)\n",
    "\n",
    "# Group the missing entries by 'col' and count\n",
    "missing_counts_by_col = missing_entries_df.groupby('col').size()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(\"Count of missing values for each column:\")\n",
    "print(missing_counts_by_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tif when Isolatrix performs fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "final_combined_df_path = \"/projects/steiflab/scratch/leli/A138856A/final_combined_df.csv\"\n",
    "log_file_path = \"/projects/steiflab/archive/data/imaging/A138856A/NozzleImages/10dropRun3/LogFile.csv\"\n",
    "\n",
    "# Read the final_combined_df.csv into a DataFrame\n",
    "final_combined_df = pd.read_csv(final_combined_df_path)\n",
    "\n",
    "# Filter the final_combined_df DataFrame based on the specified criteria\n",
    "filtered_df = final_combined_df[(final_combined_df['prediction'] == 1) & \n",
    "                                (final_combined_df['ground_truth'] == 1) & \n",
    "                                (final_combined_df['subdirectory_name'] == '10dropRun3') & \n",
    "                                (final_combined_df['technology'] == 'Isolatrix')][['row', 'col']]\n",
    "\n",
    "# Read the LogFile.csv into a DataFrame\n",
    "log_file_df = pd.read_csv(log_file_path)\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row and col combination in the filtered DataFrame\n",
    "for index, row in filtered_df.iterrows():\n",
    "    r, c = row['row'], row['col']\n",
    "    \n",
    "    # Filter the log_file_df to find matching rows and columns\n",
    "    matching_rows = log_file_df[(log_file_df['R'] == r) & (log_file_df['C'] == c)]\n",
    "    \n",
    "    # If there are matching rows, find the one with the largest file_name number\n",
    "    if not matching_rows.empty:\n",
    "        matching_rows['file_number'] = matching_rows['file_name'].str.extract('(\\d+)').astype(int)\n",
    "        max_file_row = matching_rows.loc[matching_rows['file_number'].idxmax()]\n",
    "        \n",
    "        # Store the result\n",
    "        results.append({\n",
    "            'row': r,\n",
    "            'col': c,\n",
    "            'file_name': max_file_row['file_name'],\n",
    "            'file_number': max_file_row['file_number']\n",
    "        })\n",
    "\n",
    "# Convert the results into a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results by the 'file_number' column\n",
    "sorted_results_df = results_df.sort_values(by='file_number')\n",
    "\n",
    "# Print the sorted results\n",
    "print(sorted_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
