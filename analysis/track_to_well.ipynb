{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning Track IDs to well index\n",
    "\n",
    "Each run has a csv file that outlines the last image captured before dispensing an object into a certain well. Thi can be used to detect the track that was dispensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tiff\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from skimage.measure import label, regionprops\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def load_resize_combine_display(tif_path, png_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Load a TIFF image and a PNG image, resize the PNG to match the TIFF dimensions, and combine/display both images.\n",
    "\n",
    "    Parameters:\n",
    "    - tif_path: str, path to the TIFF image file.\n",
    "    - png_path: str, path to the PNG image file.\n",
    "    - output_path: str, path to save the combined image (optional).\n",
    "    \"\"\"\n",
    "    # Load the TIFF file\n",
    "    tif_image = tiff.imread(tif_path)\n",
    "    \n",
    "    # Load the PNG file\n",
    "    png_image = Image.open(png_path)\n",
    "    \n",
    "    # Resize the PNG file to match the TIFF dimensions\n",
    "    png_resized = png_image.resize((tif_image.shape[1], tif_image.shape[0]), Image.LANCZOS)\n",
    "    \n",
    "    # Convert the resized PNG image to a numpy array\n",
    "    png_array = np.array(png_resized)\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Display the TIFF image with viridis colormap\n",
    "    ax[0].imshow(tif_image, cmap='viridis')\n",
    "    ax[0].set_title('TIFF Image (Viridis)')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # Display the resized PNG image\n",
    "    ax[1].imshow(png_array)\n",
    "    ax[1].set_title('Resized PNG Image')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the combined image if output_path is provided\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300)\n",
    "    \n",
    "    # Show the combined image\n",
    "    #plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def find_highest_object(frame):\n",
    "    \"\"\"\n",
    "    Find the object that has the highest y value within the bottom quarter of the image.\n",
    "\n",
    "    Parameters:\n",
    "    - frame: numpy array, the pixel matrix where entries are object assignments.\n",
    "\n",
    "    Returns:\n",
    "    - highest_object_id: int, the ID of the object that is closest to the bottom and within the bottom quarter of the image.\n",
    "    - highest_y: int, the highest y-coordinate of the object.\n",
    "    \"\"\"\n",
    "    # Calculate the threshold for the bottom quarter\n",
    "    threshold = 2.5 * frame.shape[0] // 4\n",
    "    \n",
    "    # Label the objects in the frame\n",
    "    unique_labels = np.unique(frame)\n",
    "    \n",
    "    # Initialize variables to keep track of the highest object\n",
    "    highest_object_id = 0\n",
    "    highest_y = 0\n",
    "    \n",
    "    # Iterate through each labeled region\n",
    "    for label in unique_labels:\n",
    "        if label == 0: \n",
    "            continue \n",
    "        #print(f\"iterating through the find highest y value fucntion and the current rehion is {label}\")\n",
    "\n",
    "        binary_mask = (frame == label).astype(np.uint8)\n",
    "        region = regionprops(binary_mask)[0]\n",
    "        # Get the coordinates of the region\n",
    "        coords = region.coords\n",
    "        # Find the maximum y-coordinate of the region\n",
    "        max_y = coords[:, 0].max()\n",
    "        #print(f\"The max y coor is {max_y} and the threshold is {threshold} with the current highest is {highest_y} for track id: {label}\")\n",
    "        # Check if the maximum y-coordinate is within the bottom quarter\n",
    "        if max_y >= threshold and max_y > highest_y:\n",
    "            highest_y = max_y\n",
    "            highest_object_id = label\n",
    "    \n",
    "    return highest_object_id, highest_y\n",
    "\n",
    "def numerical_sort(value):\n",
    "    \"\"\"\n",
    "    Extracts the numeric part from the filename for sorting.\n",
    "    Assumes that the filename format is '<number>_htert_Run'.\n",
    "    \"\"\"\n",
    "    parts = re.findall(r'\\d+', value)\n",
    "    return int(parts[0]) if parts else value\n",
    "\n",
    "\n",
    "def link_track_to_well(logfile_directory, tracked_tif_directory, linkfile_directory, output_directory=None):\n",
    "    linkfile = pd.read_csv(linkfile_directory)\n",
    "    linkfile = linkfile.sort_values(by='tifs').reset_index(drop=True)\n",
    "    logfile = pd.read_csv(logfile_directory, dtype={\"file_name\": str, \"R\": int, \"C\": int, \"Number_of_droplets\": int, \"Model_output0\": float, \"Model_output1\": float, \"Model_output2\": float, \"Prediction\": int})\n",
    "    gr_logfile = logfile.groupby(['R', 'C'])\n",
    "\n",
    "    final_dict = {\"track_ID\": [], \n",
    "                  \"after_dispense_frame\": [], \n",
    "                  \"last_tracked_frame\": [],\n",
    "                  \"last_tracked_tif\": [],\n",
    "                  \"row\": [],\n",
    "                  \"col\": []}\n",
    "\n",
    "    for (r, c), group in gr_logfile:\n",
    "        # Establish the values to be inputted into the dataframe\n",
    "        track_ID = 0  # Default value meaning no track associated\n",
    "        after_dispense_frame = sorted(group['file_name'].tolist(), key=numerical_sort, reverse=True)[0]  # This is the last frame for this (r, c) well\n",
    "        last_tracked_frame = after_dispense_frame \n",
    "        last_tracked_tif = \"\"  # Empty because the image may not have a corresponding tif file\n",
    "\n",
    "        # Obtain the 3 frames right before the frame after dispense \n",
    "        tif_to_consider = []\n",
    "        img_to_consider = []\n",
    "        frames_before = [f for f in linkfile[\"imgs\"] if numerical_sort(f) <= numerical_sort(after_dispense_frame)]\n",
    "\n",
    "        if len(frames_before) > 0: \n",
    "            prev_closest_frame = sorted(frames_before, key=numerical_sort, reverse=True)[0]\n",
    "            tif_after_dispense = linkfile.loc[linkfile[\"imgs\"] == prev_closest_frame][\"tifs\"]\n",
    "\n",
    "            if len(tif_after_dispense) != 0:\n",
    "                i = tif_after_dispense.index[0]\n",
    "                if i >= 4 and numerical_sort(after_dispense_frame) > numerical_sort(prev_closest_frame): \n",
    "                    tif_to_consider = sorted(linkfile.iloc[i-4:i+1]['tifs'], key=numerical_sort, reverse=True)\n",
    "                    img_to_consider = sorted(linkfile.iloc[i-4:i+1]['imgs'], key=numerical_sort, reverse=True)\n",
    "                elif i >= 4: \n",
    "                    tif_to_consider = sorted(linkfile.iloc[i-4:i]['tifs'], key=numerical_sort, reverse=True)\n",
    "                    img_to_consider = sorted(linkfile.iloc[i-4:i]['imgs'], key=numerical_sort, reverse=True)\n",
    "                elif i != 0: \n",
    "                    tif_to_consider = sorted(linkfile.iloc[:i]['tifs'], key=numerical_sort, reverse=True)\n",
    "                    img_to_consider = sorted(linkfile.iloc[:i]['imgs'], key=numerical_sort, reverse=True)\n",
    "\n",
    "            for tif_name, img_name in zip(tif_to_consider, img_to_consider):\n",
    "                tracked_tif = tiff.imread(os.path.join(tracked_tif_directory, tif_name))\n",
    "                track_ID, _ = find_highest_object(tracked_tif)\n",
    "                last_tracked_frame = img_name\n",
    "                last_tracked_tif = tif_name\n",
    "                if track_ID:\n",
    "                    break  # Stop if we find one that fits into our threshold\n",
    "\n",
    "        # Append the results to the final dictionary\n",
    "        final_dict[\"track_ID\"].append(int(track_ID))\n",
    "        final_dict[\"after_dispense_frame\"].append(after_dispense_frame)\n",
    "        final_dict[\"last_tracked_frame\"].append(last_tracked_frame)\n",
    "        final_dict[\"last_tracked_tif\"].append(last_tracked_tif)\n",
    "        final_dict[\"row\"].append(r)\n",
    "        final_dict[\"col\"].append(c)\n",
    "\n",
    "    if output_directory: \n",
    "        pd.DataFrame(final_dict).to_csv(os.path.join(output_directory, \"track_to_well_unfiltered.csv\"), index=False)\n",
    "    \n",
    "    return pd.DataFrame(final_dict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"A138856A\"\n",
    "printrun = \"10dropRun4\"\n",
    "logfile_directory = f'/projects/steiflab/archive/data/imaging/{dataset}/NozzleImages/{printrun}/LogFile.csv'\n",
    "tracked_tif_directory = f'/projects/steiflab/scratch/leli/trackastra/{dataset}/{printrun}/tracked_again_postprocessed_2.0'\n",
    "fluro_directory = f'/projects/steiflab/archive/data/imaging/{dataset}/MicroscopeImages/S0000/'\n",
    "linkfile_directory = f'/projects/steiflab/scratch/leli/trackastra/{dataset}/{printrun}/tracked_again_postprocessed_2.0/tif_to_img.csv'\n",
    "\n",
    "df = link_track_to_well(logfile_directory, tracked_tif_directory, linkfile_directory, output_directory = f'/projects/steiflab/scratch/leli/{dataset}/{printrun}/track_to_well')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 18\n",
    "c = 36\n",
    "linkfile = pd.read_csv(linkfile_directory)\n",
    "linkfile = linkfile.sort_values(by='tifs').reset_index(drop=True)\n",
    "# print(linkfile)\n",
    "logfile = pd.read_csv(logfile_directory, dtype={\"file_name\": str, \"R\": int, \"C\": int, \"Number_of_droplets\": int, \"Model_output0\": float, \"Model_output1\": float, \"Model_output2\": float, \"Prediction\": int})\n",
    "gr_logfile = logfile.groupby(['R', 'C'])\n",
    "group = gr_logfile.get_group((r, c))\n",
    "\n",
    "fluro_img = tiff.imread(os.path.join(fluro_directory, f\"C{c:02d}\", f\"R{r:02d}_C{c:02d}_0000_00_Cyan.tif\"))\n",
    "\n",
    "\n",
    "# establish the values to be inputted into the dataframe\n",
    "track_ID = 0 # this is default meaning no track associated\n",
    "after_dispense_frame = sorted(group['file_name'].tolist(), key = numerical_sort, reverse=True)[0] # This is the last frame for this (r,c) well \n",
    "last_tracked_frame = after_dispense_frame \n",
    "last_tracked_tif = \"\" # empty because the image may not have a correspnding tif file \n",
    "prediction = np.max(group['Prediction'].tolist())\n",
    "if group.loc[group['Prediction'] > 0].shape[0] > 1: raise ValueError(f\"there are multiple predicitons that are greater than 0 {group['Prediction']}\")\n",
    "fluro_image = os.path.join(f\"C{c:02d}\", f\"R{r:02d}_C{c:02d}_0000_00_Cyan.tif\")\n",
    "\n",
    "\n",
    "# here we implement the label which are determined by the fluorescent images and the model prediction \n",
    "if prediction == 0: \n",
    "    is_fluro = \"NCC\"\n",
    "elif prediction == 1: \n",
    "    is_fluro = \"singlecell\"\n",
    "    if np.max(np.array(fluro_img)) < 2000: \n",
    "        is_fluro = \"deadcell\"\n",
    "elif prediction == 2: \n",
    "    if np.max(np.array(fluro_img)) >= 2000:\n",
    "        is_fluro = \"debris\"\n",
    "    else: \n",
    "        is_fluro = \"multicell\"\n",
    "\n",
    "#is_fluro = \"livecell\" if np.max(np.array(fluro_img)) >= 2000 else \"NCC_debris\"\n",
    "\n",
    "# obtain the 3 frames right before the frame after dispense \n",
    "tif_to_consider = []\n",
    "img_to_consider = []\n",
    "frames_before = [f for f in linkfile[\"imgs\"] if numerical_sort(f) <= numerical_sort(after_dispense_frame)]\n",
    "if len(frames_before) == 0: \n",
    "    print(f\"There are zero frames including the current after dispense frame one {after_dispense_frame}\")\n",
    "else: \n",
    "    prev_closest_frame = sorted(frames_before, key = numerical_sort, reverse = True)[0]\n",
    "    print(f\"prev_closest_frame is {prev_closest_frame}\")\n",
    "    tif_after_dispense = linkfile.loc[linkfile[\"imgs\"] == prev_closest_frame][\"tifs\"]\n",
    "    if len(tif_after_dispense) != 0:\n",
    "        i = tif_after_dispense.index[0]\n",
    "        #print(f\"The index of the tif after dispense is {i}\")\n",
    "        if i >= 4 and numerical_sort(after_dispense_frame) > numerical_sort(prev_closest_frame): \n",
    "            tif_to_consider = sorted(linkfile.iloc[i-4:i+1]['tifs'], key = numerical_sort, reverse = True)\n",
    "            img_to_consider = sorted(linkfile.iloc[i-4:i+1]['imgs'], key = numerical_sort, reverse = True)\n",
    "        elif i >= 4: \n",
    "            tif_to_consider = sorted(linkfile.iloc[i-4:i]['tifs'], key = numerical_sort, reverse = True)\n",
    "            img_to_consider = sorted(linkfile.iloc[i-4:i]['imgs'], key = numerical_sort, reverse = True)\n",
    "        elif i != 0: \n",
    "            tif_to_consider = sorted(linkfile.iloc[:i]['tifs'], key = numerical_sort, reverse = True)\n",
    "            img_to_consider = sorted(linkfile.iloc[:i]['imgs'], key = numerical_sort, reverse = True)\n",
    "    print(f\"tif_to_consider: {tif_to_consider}\")\n",
    "    for tif_name, img_name in zip(tif_to_consider, img_to_consider):\n",
    "\n",
    "        tracked_tif = tiff.imread(os.path.join(tracked_tif_directory, tif_name))\n",
    "        print(f\"the tracked tif file is {tif_name}\")\n",
    "\n",
    "        track_ID, highest_y = find_highest_object(tracked_tif)\n",
    "        last_tracked_frame = img_name\n",
    "        last_tracked_tif = tif_name\n",
    "        if track_ID: break # since the filename is sorted from later frames to earlier, if we find one that fits into our treshold then we stop \n",
    "\n",
    "print(f\"The print ID ends up being : {track_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we have the df ready where we can get some analysis done and see what is happening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/projects/steiflab/scratch/leli/trackastra/track_to_well/track_to_well.csv')\n",
    "# Filter the DataFrame for rows where 'track_ID' is 0 and 'type' is 'livecell'\n",
    "livecell_track0 = df[(df['track_ID'] == 0) & (df['type'] == 'livecell')]\n",
    "\n",
    "# Get the number of such rows\n",
    "num_livecell_track0 = livecell_track0.shape[0]\n",
    "\n",
    "# Get the total number of 'livecell' types\n",
    "total_livecell = df[df['type'] == 'livecell'].shape[0]\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_livecell_track0 = (num_livecell_track0 / total_livecell) * 100\n",
    "\n",
    "print(f\"Number of 'livecell' types with a track_ID of 0: {num_livecell_track0}\")\n",
    "print(f\"Total number of 'livecell' types: {total_livecell}\")\n",
    "print(f\"Percentage of 'livecell' types with a track_ID of 0: {percentage_livecell_track0:.2f}%\")\n",
    "\n",
    "# Get the corresponding 'last_tracked_tif' values\n",
    "last_tracked_tif_values = livecell_track0['last_tracked_tif']\n",
    "\n",
    "# Print the 'last_tracked_tif' values\n",
    "print(\"Corresponding 'last_tracked_tif' values for 'livecell' types with a track_ID of 0:\")\n",
    "for tif in last_tracked_tif_values:\n",
    "    print(tif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: How many livecells do not have a track to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.loc[(df['track_ID'] == 0) & (df['type'] == 'singlecell')]['fluro_image'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the counts of each type\n",
    "type_counts = df['type'].value_counts()\n",
    "\n",
    "# Calculate the percentages of each type\n",
    "type_percentages = df['type'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the counts and percentages\n",
    "print(\"Counts and Percentages of each type:\")\n",
    "for type_value, count in type_counts.items():\n",
    "    percentage = type_percentages[type_value]\n",
    "    print(f\"{type_value}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "# Define the number of samples to take from each type\n",
    "n_samples = 10\n",
    "\n",
    "# Function to print the last_tracked_tif values for each type\n",
    "def print_random_samples(df, n_samples):\n",
    "    unique_types = df['type'].unique()\n",
    "    for type_value in unique_types:\n",
    "        # Filter the dataframe for the current type\n",
    "        type_df = df[df['type'] == type_value]\n",
    "        \n",
    "        # Randomly sample n_samples rows from the filtered dataframe\n",
    "        sampled_df = type_df.sample(n=min(n_samples, len(type_df)), random_state=42)\n",
    "        \n",
    "        # Print the last_tracked_tif values for the sampled rows\n",
    "        print(f\"Type: {type_value}\")\n",
    "        print(sampled_df['last_tracked_tif'].tolist())\n",
    "        print()\n",
    "\n",
    "# Call the function with your dataframe\n",
    "print_random_samples(df, n_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just to update the track info file, the test version is just to make sure not overwrite the previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_track_info_across_frame(old_track_info_df, track_info_df, frame, frame_number):\n",
    "    unique_labels = np.unique(frame)\n",
    "    unique_labels = unique_labels[unique_labels != 0]  # Remove background (label 0)\n",
    "\n",
    "    for track_id in unique_labels:\n",
    "        row = track_info_df[track_info_df['Track_ID'] == track_id]\n",
    "\n",
    "        if not row.empty:\n",
    "            start_frame = int(row['Start'].values[0])\n",
    "            end_frame = int(row['End'].values[0])\n",
    "            start_frame = min(start_frame, frame_number)\n",
    "            end_frame = max(end_frame, frame_number)\n",
    "            track_info_df.loc[track_info_df['Track_ID'] == track_id, 'Start'] = start_frame\n",
    "            track_info_df.loc[track_info_df['Track_ID'] == track_id, 'End'] = end_frame\n",
    "        else:\n",
    "            parent_value = old_track_info_df.loc[old_track_info_df['Track_ID'] == track_id, 'Parent'].values[0] if track_id in old_track_info_df['Track_ID'].values else 0\n",
    "            new_row = pd.DataFrame({\n",
    "                'Track_ID': [track_id],\n",
    "                'Start': [frame_number],\n",
    "                'End': [frame_number],\n",
    "                'Parent': [parent_value]\n",
    "            })\n",
    "            track_info_df = pd.concat([track_info_df, new_row], ignore_index=True)\n",
    "    \n",
    "    return track_info_df\n",
    "    \n",
    "track_info_file = '/projects/steiflab/scratch/leli/trackastra/A138974A/PrintRun_Apr1223_1311/tracked_postprocessed_2.0/man_track.txt'\n",
    "target_tif_dir = '/projects/steiflab/scratch/leli/trackastra/A138974A/PrintRun_Apr1223_1311/tracked_postprocessed_2.0'\n",
    "track_info = pd.read_csv(track_info_file, sep='\\s+', names=['Track_ID', 'Start', 'End', 'Parent']) # read in again the new versin of track info \n",
    "new_track_info = pd.DataFrame(columns=track_info.columns)\n",
    "for filename in os.listdir(target_tif_dir):\n",
    "    if not filename.startswith(\"._\") and filename.endswith(\".tif\"):\n",
    "        frame_path = os.path.join(target_tif_dir, filename)\n",
    "        frame = tiff.imread(frame_path)\n",
    "\n",
    "        frame_num = int(filename.replace('man_track', '').replace('.tif', ''))\n",
    "        new_track_info = update_track_info_across_frame(track_info, new_track_info, frame, frame_num)\n",
    "        new_track_info.to_csv(os.path.join(target_tif_dir, \"man_track_test.txt\"), sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we post process the df and make sure to eliminate duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual function where we process the track to well df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[579, 577, 579, 579]\n",
      "[1655, 1660, 1660]\n",
      "[990, 990, 990, 990, 990]\n",
      "[323, 324, 324, 324, 324, 324]\n",
      "[1057, 1057]\n",
      "[74, 77, 83, 86, 117, 100, 99]\n",
      "[1326, 1358, 1341, 1341]\n",
      "[629, 629, 629]\n",
      "[907, 907]\n",
      "[1752, 1752, 1752]\n",
      "[1432, 1432]\n",
      "[670, 669]\n",
      "[1265, 1265]\n",
      "[1712, 1719, 1722]\n",
      "[353, 353]\n",
      "[518]\n",
      "[234]\n",
      "[267, 268]\n",
      "[924, 924]\n",
      "[1039, 1039, 1039]\n",
      "[1816, 1816]\n",
      "[771, 777]\n",
      "[1212]\n",
      "[1464, 1465]\n",
      "[1180]\n",
      "[1105]\n",
      "[751]\n",
      "[891]\n",
      "[1249]\n",
      "[1027]\n",
      "[1554]\n",
      "[1075]\n",
      "[1495]\n",
      "[1564]\n",
      "[1262]\n",
      "[1577]\n",
      "[1597]\n",
      "[1300]\n",
      "[1642]\n",
      "[1415]\n",
      "[1384]\n",
      "[1367]\n",
      "[1537, 1537]\n",
      "[1692]\n",
      "[154]\n",
      "[340]\n",
      "[483]\n",
      "[517]\n",
      "[566]\n",
      "[615]\n",
      "[253]\n",
      "[694]\n",
      "[382]\n",
      "[801]\n",
      "[31, 31]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def get_group(type_value):\n",
    "    if type_value in ['error0', 'NCC']:\n",
    "        return 0\n",
    "    elif type_value in ['error1', 'singlecell']:\n",
    "        return 1\n",
    "    elif type_value in ['debris', 'cluster']:\n",
    "        return 2\n",
    "    else:\n",
    "        return -1\n",
    "        \n",
    "def process_track_ids(df, track_info, output_directory):\n",
    "\n",
    "    # Process each unique track ID with multiple rows\n",
    "    track_id_counts = df['track_ID'].value_counts()\n",
    "    multiple_row_track_ids = track_id_counts[track_id_counts > 1].index\n",
    "\n",
    "    for track_id in multiple_row_track_ids:\n",
    "        if track_id <= 0: continue\n",
    "\n",
    "        track_df = df[df['track_ID'] == track_id]\n",
    "        \n",
    "        # Determine the group of each row\n",
    "        groups = track_df['type'].apply(get_group)\n",
    "        \n",
    "        if len(groups.unique()) == 1:\n",
    "            group = groups.iloc[0]\n",
    "            \n",
    "            if group == 0:\n",
    "                print(f\"Track ID {track_id}: All rows are from group 0 (error0 and NCC).\")\n",
    "            elif group in [1, 2]:\n",
    "                track_info_row = track_info[track_info['Track_ID'] == track_id]\n",
    "                if track_info_row.empty:\n",
    "                    print(f\"Track ID {track_id}: No matching track info found.\")\n",
    "                    continue\n",
    "\n",
    "                end_value = track_info_row['End'].values[0]\n",
    "                df.loc[track_df.index, 'distance_to_end'] = [abs(numerical_sort(i) - end_value) for i in track_df['last_tracked_tif']]\n",
    "                closest_row_idx = df.loc[track_df.index, 'distance_to_end'].idxmin()\n",
    "                \n",
    "                if group == 1:\n",
    "                    new_track_id = -1\n",
    "                else:  # group == 2\n",
    "                    new_track_id = -2\n",
    "                \n",
    "                df.loc[track_df.index.difference([closest_row_idx]), 'track_ID'] = new_track_id\n",
    "                #df.drop(columns=['distance_to_end'], inplace=True)\n",
    "                \n",
    "        else:\n",
    "            track_df_group1_2 = track_df[groups.isin([1, 2])]\n",
    "            if not track_df_group1_2.empty:\n",
    "                track_info_row = track_info[track_info['Track_ID'] == track_id]\n",
    "                if track_info_row.empty:\n",
    "                    print(f\"Track ID {track_id}: No matching track info found.\")\n",
    "                    continue\n",
    "\n",
    "                end_value = track_info_row['End'].values[0]\n",
    "                print([numerical_sort(i) for i in track_df_group1_2['last_tracked_tif']] )\n",
    "                df.loc[track_df_group1_2.index, 'distance_to_end'] = [abs(numerical_sort(i) - end_value) for i in track_df_group1_2['last_tracked_tif']]\n",
    "                closest_row_idx = df.loc[track_df_group1_2.index, 'distance_to_end'].idxmin()\n",
    "\n",
    "\n",
    "                df.loc[track_df.index.difference([closest_row_idx]), 'track_ID'] = -3\n",
    " \n",
    "    if output_directory: df.to_csv(os.path.join(output_directory, \"track_to_well.csv\"), index = False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "dataset = \"A138856A\"\n",
    "printrun = \"10dropRun4\"\n",
    "df = pd.read_csv(f\"/projects/steiflab/scratch/leli/{dataset}/{printrun}/track_to_well/track_to_well.csv\")\n",
    "new_track_info = pd.read_csv(f\"/projects/steiflab/scratch/leli/trackastra/{dataset}/{printrun}/tracked_again_postprocessed_2.0/man_track.txt\",sep='\\s+', names=['Track_ID', 'Start', 'End', 'Parent'])\n",
    "processed_df = process_track_ids(df, new_track_info, output_directory = f'/projects/steiflab/scratch/leli/{dataset}/{printrun}/track_to_well')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRECTED !!!!!!! VERSION 2 Here we post process the df and make sure to eliminate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_track_ids_by_distance(df, track_info, output_directory=None):\n",
    "    # Process each unique track ID with multiple rows\n",
    "    track_id_counts = df['track_ID'].value_counts()\n",
    "    multiple_row_track_ids = track_id_counts[track_id_counts > 1].index\n",
    "\n",
    "    for track_id in multiple_row_track_ids:\n",
    "        if track_id <= 0: \n",
    "            continue\n",
    "\n",
    "        track_df = df[df['track_ID'] == track_id]\n",
    "\n",
    "        # Find the corresponding track info row\n",
    "        track_info_row = track_info[track_info['Track_ID'] == track_id]\n",
    "        if track_info_row.empty:\n",
    "            print(f\"Track ID {track_id}: No matching track info found.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the distance to the end value for each row\n",
    "        end_value = track_info_row['End'].values[0]\n",
    "        df.loc[track_df.index, 'distance_to_end'] = [abs(numerical_sort(i) - end_value) for i in track_df['last_tracked_tif']]\n",
    "\n",
    "        # Find the row with the minimum distance to the end\n",
    "        closest_row_idx = df.loc[track_df.index, 'distance_to_end'].idxmin()\n",
    "\n",
    "        # Label all other rows with -1 in the track_ID column\n",
    "        df.loc[track_df.index.difference([closest_row_idx]), 'track_ID'] = -1\n",
    "\n",
    "    # Save the final DataFrame if an output directory is provided\n",
    "    if output_directory: \n",
    "        if \"distance_to_end\" in list(df.columns): df.drop(columns=['distance_to_end'], inplace=True)\n",
    "        df.to_csv(os.path.join(output_directory, \"track_to_well_pp.csv\"), index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "'''dataset = \"A138856A\"\n",
    "printrun = \"10dropRun1\"'''\n",
    "df = pd.read_csv(f\"/projects/steiflab/scratch/leli/{dataset}/{printrun}/track_to_well/track_to_well_unfiltered.csv\")\n",
    "new_track_info = pd.read_csv(f\"/projects/steiflab/scratch/leli/trackastra/{dataset}/{printrun}/tracked_again_postprocessed_2.0/man_track.txt\", sep='\\s+', names=['Track_ID', 'Start', 'End', 'Parent'])\n",
    "processed_df = process_track_ids_by_distance(df, new_track_info, output_directory=f'/projects/steiflab/scratch/leli/{dataset}/{printrun}/track_to_well')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we assess if the all track ids have 1 row instead of multiple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique positive non-zero track IDs and their counts:\n",
      "track_ID\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "Name: count, dtype: int64\n",
      "All positive non-zero track IDs have only one row.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_ID</th>\n",
       "      <th>after_dispense_frame</th>\n",
       "      <th>last_tracked_frame</th>\n",
       "      <th>last_tracked_tif</th>\n",
       "      <th>distance_to_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [track_ID, after_dispense_frame, last_tracked_frame, last_tracked_tif, distance_to_end]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_unique_track_ids(df):\n",
    "    # Filter out non-zero track IDs\n",
    "    positive_nonzero_track_ids = df[df['track_ID'] > 0]['track_ID']\n",
    "\n",
    "    # Get the value counts of these track IDs\n",
    "    track_id_counts = positive_nonzero_track_ids.value_counts()\n",
    "\n",
    "    # Display the track IDs and their counts\n",
    "    print(\"Unique positive non-zero track IDs and their counts:\")\n",
    "    print(track_id_counts)\n",
    "\n",
    "    # Verify that each track ID has only one row\n",
    "    duplicate_tracks = track_id_counts[track_id_counts > 1]\n",
    "    if duplicate_tracks.empty:\n",
    "        print(\"All positive non-zero track IDs have only one row.\")\n",
    "    else:\n",
    "        print(\"Some track IDs have duplicates:\")\n",
    "        print(duplicate_tracks)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "display_unique_track_ids(processed_df)\n",
    "processed_df[processed_df['track_ID'] == 147]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
